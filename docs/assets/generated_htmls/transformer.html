                                <div class="section-block-small">
                                    <div class="doc-block">
                                        <table>
                                           <td><span class="label">&nbsp;CLS</span></td>
                                            <td><span class="name">PositionalEncoding(nn.Module)</span></td>
                                            <td class="expand"><span class="params">(d)</span></td>
                                        </table>
                                        <p><span class="desc">Positional encoding layer using sinusoid curve.</span></p>
                                        <ul class="list">
                                           <li><b>d</b>: shaping parameter for the sinusoid</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="section-block-small">
                                    <div class="doc-block">
                                        <table>
                                           <td><span class="label">&nbsp;CLS</span></td>
                                            <td><span class="name">TransformerEmbedding(nn.Module)</span></td>
                                            <td class="expand"><span class="params">(vocab_size, emb_size, dropout=0.)</span></td>
                                        </table>
                                        <p><span class="desc">Transformer embedding layer (embedding + positional encoding + dropout).</span></p>
                                        <ul class="list">
                                           <li><b>vocab_size</b>: vocabulary size</li>
                                           <li><b>emb_size</b>: embedding size</li>
                                           <li><b>dropout</b>: dropout factor (0 to 1)</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="section-block-small">
                                    <div class="doc-block">
                                        <table>
                                           <td><span class="label">&nbsp;DEF</span></td>
                                            <td><span class="name">feed_forward</span></td>
                                            <td class="expand"><span class="params">(model_dim, inner_dim, drop_ff=0., double_drop=True)</span></td>
                                        </table>
                                        <p><span class="desc">Feed forward layer after multi-headed attention (2 * Linear + skip connection + layer norm).</span></p>
                                        <ul class="list">
                                           <li><b>model_dim</b>: input dimension input feedforward nn</li>
                                           <li><b>inner_dim</b>: number of hidden units in the first linear layer in feedforward nn</li>
                                           <li><b>drop_ff</b>: dropout factor (0 to 1)</li>
                                           <li><b>double_drop</b>: whether to have dropout layer after first linear layer in feedforward nn</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="section-block-small">
                                    <div class="doc-block">
                                        <table>
                                           <td><span class="label">&nbsp;CLS</span></td>
                                            <td><span class="name">MultiHeadAttention(nn.Module)</span></td>
                                            <td class="expand"><span class="params">(num_head, model_dim, head_dim=None, drop_res=0., drop_att=0., bias=True, scale=True)</span></td>
                                        </table>
                                        <p><span class="desc">Multi-Head Attention with key, query, value.</span></p>
                                        <ul class="list">
                                           <li><b>num_head</b>: number of attention head</li>
                                           <li><b>model_dim</b>: input dimension input feedforward nn</li>
                                           <li><b>head_dim</b>: dimension of each attention head</li>
                                           <li><b>drop_res</b>: dropout factor (0 to 1) for output of attentions</li>
                                           <li><b>drop_att</b>: dropout factor (0 to 1) for attention layers</li>
                                           <li><b>bias</b>: whether to allow bias for linear layers</li>
                                           <li><b>scale</b>: scaling factor for attention scores</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="section-block-small">
                                    <div class="doc-block">
                                        <table>
                                           <td><span class="label">&nbsp;CLS</span></td>
                                            <td><span class="name">TransEncoder(nn.Module)</span></td>
                                            <td class="expand"><span class="params">(num_head, model_dim, head_dim, inner_dim, drop_res=0., drop_att=0., drop_ff=0., bias=True, scale=True, double_drop=True)</span></td>
                                        </table>
                                        <p><span class="desc">Encoder block of a Transformer model.</span></p>
                                        <ul class="list">
                                           <li><b>num_head</b>: number of attention head</li>
                                           <li><b>model_dim</b>: input dimension input feedforward nn</li>
                                           <li><b>head_dim</b>: dimension of each attention head</li>
                                           <li><b>inner_dim</b>: number of hidden units in the first linear layer in feedforward nn</li>
                                           <li><b>drop_res</b>: dropout factor (0 to 1) for output of attentions</li>
                                           <li><b>drop_att</b>: dropout factor (0 to 1) for attention layers</li>
                                           <li><b>drop_ff</b>:  dropout factor (0 to 1) for feedforward layers</li>
                                           <li><b>bias</b>: whether to allow bias for linear layers</li>
                                           <li><b>scale</b>: scaling factor for attention scores</li>
                                           <li><b>double_drop</b>: whether to have dropout layer after first linear layer in feedforward nn</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="section-block-small">
                                    <div class="doc-block">
                                        <table>
                                           <td><span class="label">&nbsp;CLS</span></td>
                                            <td><span class="name">TransDecoder(nn.Module)</span></td>
                                            <td class="expand"><span class="params">(num_head, model_dim, head_dim, inner_dim, drop_res=0., drop_att=0., drop_ff=0., bias=True, scale=True, double_drop=True)</span></td>
                                        </table>
                                        <p><span class="desc">Decoder block of a Transformer model.</span></p>
                                        <ul class="list">
                                           <li><b>num_head</b>: number of attention head</li>
                                           <li><b>model_dim</b>: input dimension input feedforward nn</li>
                                           <li><b>head_dim</b>: dimension of each attention head</li>
                                           <li><b>inner_dim</b>: number of hidden units in the first linear layer in feedforward nn</li>
                                           <li><b>drop_res</b>: dropout factor (0 to 1) for output of attentions</li>
                                           <li><b>drop_att</b>: dropout factor (0 to 1) for attention layers</li>
                                           <li><b>drop_ff</b>:  dropout factor (0 to 1) for feedforward layers</li>
                                           <li><b>bias</b>: whether to allow bias for linear layers</li>
                                           <li><b>scale</b>: scaling factor for attention scores</li>
                                           <li><b>double_drop</b>: whether to have dropout layer after first linear layer in feedforward nn</li>
                                        </ul>
                                    </div>
                                </div>
                                <div class="section-block-small">
                                    <div class="doc-block">
                                        <table>
                                           <td><span class="label">&nbsp;CLS</span></td>
                                            <td><span class="name">Transformer(nn.Module)</span></td>
                                            <td class="expand"><span class="params">(vocab_size, out_size, n_layers=6, num_head=8, model_dim=256, head_dim=32, inner_dim=1024, drop_inp=0.1, drop_res=0.1, drop_att=0.1, drop_ff=0.1, bias=True, scale=True, double_drop=True, pad_idx=1)</span></td>
                                        </table>
                                        <p><span class="desc">Transformer model.</span></p>
                                        <ul class="list">
                                           <li><b>vocab_size</b>: vocabulary size</li>
                                           <li><b>out_size</b>: output numbers (number of labels)</li>
                                           <li><b>n_layers</b>: number of transformer encoder and decoders</li>
                                           <li><b>num_head</b>: number of attention head</li>
                                           <li><b>model_dim</b>: input dimension input feedforward nn</li>
                                           <li><b>head_dim</b>: dimension of each attention head</li>
                                           <li><b>inner_dim</b>: number of hidden units in the first linear layer in feedforward nn</li>
                                           <li><b>drop_inp</b>: dropout factor (0 tp 1) for embedding layer</li>
                                           <li><b>drop_res</b>: dropout factor (0 to 1) for output of attentions</li>
                                           <li><b>drop_att</b>: dropout factor (0 to 1) for attention layers</li>
                                           <li><b>drop_ff</b>:  dropout factor (0 to 1) for feedforward layers</li>
                                           <li><b>bias</b>: whether to allow bias for linear layers</li>
                                           <li><b>scale</b>: scaling factor for attention scores</li>
                                           <li><b>double_drop</b>: whether to have dropout layer after first linear layer in feedforward nn</li>
                                           <li><b>pad_idx</b>: padding index for input masking</li>
                                        </ul>
                                    </div>
                                </div>