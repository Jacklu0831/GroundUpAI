{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(line):\n",
    "    '''return type'''\n",
    "    if line[:3] == 'def':\n",
    "        return 'def'\n",
    "    if line[:5] == 'class':\n",
    "        return 'class'\n",
    "    return None\n",
    "\n",
    "def script2doc(f_name):\n",
    "    '''Parse single script to doc'''\n",
    "    def process_function():\n",
    "        '''Parse function'''\n",
    "        nonlocal start, docs\n",
    "        name = f[start].split('(', 1)[0].split()[-1]\n",
    "        end = start\n",
    "        while end < len(f):\n",
    "            if f[end].strip().startswith(\"'''\"):\n",
    "                break\n",
    "            end += 1\n",
    "        comment = f[end][:-1].strip()[3:-3]\n",
    "        params = ''.join(f[start:end])\n",
    "        params = params.split('(', 1)[1].rsplit(')', 1)[0].replace(' ', '').replace('\\n', '').split(',')\n",
    "        print(f'name   : {name}')\n",
    "        print(f'params : {params}')\n",
    "        print(f'comment: {comment}')\n",
    "        docs.append(('def', name, params, comment))\n",
    "        start = end + 1\n",
    "        \n",
    "    def process_class():\n",
    "        '''Parse class'''\n",
    "        nonlocal start, docs\n",
    "        name = f[start].split()[1].rsplit(')')[0] + ')'\n",
    "        \n",
    "        end = start\n",
    "        while end < len(f):\n",
    "            if f[end].strip().startswith(\"'''\"):\n",
    "                break\n",
    "            end += 1\n",
    "        comment = f[end][:-1].strip()[3:-3]\n",
    "        end += 1\n",
    "        \n",
    "        start = -1\n",
    "        while end < len(f):\n",
    "            if 'def __init__' in f[end] and start == -1:\n",
    "                start = end\n",
    "            if ')' in f[end]:\n",
    "                break\n",
    "            end += 1\n",
    "        if start == -1: # no __init__\n",
    "            params = []\n",
    "        else:\n",
    "            params = ''.join(f[start:end+1])\n",
    "            params = params.split('(', 1)[1].rsplit(')', 1)[0].replace(' ', '').replace('\\n', '').split(',')[1:]\n",
    "        print(f'name   : {name}')\n",
    "        print(f'params : {params}')\n",
    "        print(f'comment: {comment}')\n",
    "        docs.append(('class', name, params, comment))\n",
    "        start = end + 1\n",
    "        \n",
    "    docs = []\n",
    "    with open(f_name, 'r') as f:\n",
    "        f = list(f)\n",
    "        start = 0\n",
    "        while start < len(f):\n",
    "            type_ = get_type(f[start])\n",
    "            if type_ == 'def':\n",
    "                print(f'{f[start][:-1]}')\n",
    "                process_function()\n",
    "                print()\n",
    "            elif type_ == 'class':\n",
    "                print(f'{f[start][:-1]}')\n",
    "                process_class()\n",
    "                print()\n",
    "            start += 1\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean():\n",
    "    '''Purge generated html directory'''\n",
    "    files = glob.glob('documentation/assets/htmls/*')\n",
    "    for f_name in files:\n",
    "        if f_name.endswith('txt'):\n",
    "            os.remove(f_name)\n",
    "\n",
    "def scripts2docs():\n",
    "    '''Parse all scripts and return a mapping from script name to doc'''\n",
    "    all_docs = {}\n",
    "#     all_docs['scripts/utils.py'] = script2doc('scripts/utils.py')\n",
    "    for name in glob.glob('scripts/*'):\n",
    "        if name.endswith('py'):\n",
    "            all_docs[name] = script2doc(name)\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = '''\n",
    "                                <div class=\"section-block-small\">\n",
    "                                    <div class=\"doc-block\">\n",
    "                                        <table>\n",
    "                                           <td><span class=\"label\">&nbsp;{}</span></td>\n",
    "                                            <td><span class=\"name\">{}</span></td>\n",
    "                                            <td class=\"expand\"><span class=\"params\">({})</span></td>\n",
    "                                        </table>\n",
    "                                        <p><span class=\"desc\">{}</span></p>\n",
    "                                    </div>\n",
    "                                </div>'''\n",
    "\n",
    "\n",
    "template2_head = '''                                <div class=\"section-block-small\">\n",
    "                                    <div class=\"doc-block\">\n",
    "                                        <table>\n",
    "                                           <td><span class=\"label\">&nbsp;{}</span></td>\n",
    "                                            <td><span class=\"name\">{}</span></td>\n",
    "                                            <td class=\"expand\"><span class=\"params\">({})</span></td>\n",
    "                                        </table>\n",
    "                                        <p><span class=\"desc\">{}</span></p>\n",
    "                                        <ul class=\"list\">'''\n",
    "\n",
    "template2_tail = '''\n",
    "                                        </ul>\n",
    "                                    </div>\n",
    "                                </div>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2html(doc):\n",
    "    '''Convert single doc to html'''\n",
    "    html = []\n",
    "    for type_, name, params, comment in doc:\n",
    "        type_ = 'DEF' if type_ == 'def' else 'CLS'\n",
    "        if not params or len(params)==1 and not params[0]:\n",
    "            html.append(template1.format(type_, name, ', '.join(params), comment))\n",
    "        else:\n",
    "            cur = template2_head.format(type_, name, ', '.join(params), comment)\n",
    "            for p in params:\n",
    "                cur += f'\\n                                           <li>{p.split(\"=\")[0]}: </li>'\n",
    "            cur += template2_tail\n",
    "            html.append(cur)\n",
    "    return '\\n'.join(html)\n",
    "\n",
    "def docs2htmls(all_docs):\n",
    "    '''Convert all docs to map from file name to html files'''\n",
    "    htmls = {}\n",
    "    for name, doc in all_docs.items():\n",
    "        htmls[name] = doc2html(doc)\n",
    "    return htmls\n",
    "\n",
    "def store_htmls(htmls):\n",
    "    '''Store htmls to documentation/assets/htmls'''\n",
    "    for name, html in htmls.items():\n",
    "        name = 'documentation/assets/htmls/' + name.split('/')[1][:-3] + '.html'\n",
    "        with open(name, 'w') as f:\n",
    "            f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scripts2htmls():\n",
    "    '''Wrapper function for going from python scripts to html'''\n",
    "    all_docs = scripts2docs()\n",
    "    htmls = docs2htmls(all_docs)\n",
    "    store_htmls(htmls)\n",
    "    return htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def camel2snake(name):\n",
      "name   : camel2snake\n",
      "params : ['name']\n",
      "comment: camel2snake case with regex\n",
      "\n",
      "class Callback():\n",
      "name   : Callback()\n",
      "params : []\n",
      "comment: Callback class with order\n",
      "\n",
      "class TrainEval(Callback):\n",
      "name   : TrainEval(Callback)\n",
      "params : []\n",
      "comment: Basic training <-> evaluation callback\n",
      "\n",
      "class Dataset():\n",
      "name   : Dataset()\n",
      "params : ['x_data', 'y_data']\n",
      "comment: Dataset class to store data and labels\n",
      "\n",
      "class Sampler():\n",
      "name   : Sampler()\n",
      "params : ['size', 'batch_size', 'shuffle']\n",
      "comment: Simple indices generator with option to randomly sample input data\n",
      "\n",
      "def collate(batch):\n",
      "name   : collate\n",
      "params : ['batch']\n",
      "comment: Util function to stack batches of x and y data\n",
      "\n",
      "class DataLoader():\n",
      "name   : DataLoader()\n",
      "params : ['dataset', 'sampler', 'collate_fn=collate']\n",
      "comment: Data loader class with data/label data and sampler to batch generation\n",
      "\n",
      "class DataBunch():\n",
      "name   : DataBunch()\n",
      "params : ['train_dl', 'valid_dl']\n",
      "comment: Data bunch class with both training and validation data loaders \n",
      "\n",
      "def get_data_bunch(x_train, y_train, x_valid, y_valid, batch_size):\n",
      "name   : get_data_bunch\n",
      "params : ['x_train', 'y_train', 'x_valid', 'y_valid', 'batch_size']\n",
      "comment: Util function for converting existing data to data bunch class for training\n",
      "\n",
      "class NGram():\n",
      "name   : NGram()\n",
      "params : ['n_gram', 'vocab_size=5000']\n",
      "comment: NGram class for preprocess texts\n",
      "\n",
      "def get_grams(inp, n, vocab_size=5000):\n",
      "name   : get_grams\n",
      "params : ['inp', 'n', 'vocab_size=5000']\n",
      "comment: Util function for grabbing multiple input NGrams of varying sizes\n",
      "\n",
      "def get_correct_n_grams(pre, tar, n, vocab_size=5000):\n",
      "name   : get_correct_n_grams\n",
      "params : ['pre', 'tar', 'n', 'vocab_size=5000']\n",
      "comment: Compute number of matching n-grams between two sentences\n",
      "\n",
      "def bleu(pre, tar, max_grams=4, vocab_size=5000):\n",
      "name   : bleu\n",
      "params : ['pre', 'tar', 'max_grams=4', 'vocab_size=5000']\n",
      "comment: Compute BLEU score between two sentences with length penalty\n",
      "\n",
      "def corpus_bleu_score(pres, tars, max_grams=4, vocab_size=5000):\n",
      "name   : corpus_bleu_score\n",
      "params : ['pres', 'tars', 'max_grams=4', 'vocab_size=5000']\n",
      "comment: Compute BLEU score between two list of sentences (corpus) with length penalty\n",
      "\n",
      "class BLEUScore(Callback):\n",
      "name   : BLEUScore(Callback)\n",
      "params : ['max_grams=4', 'vocab_size=5000']\n",
      "comment: Callback to compute BLEU score for training NLP models\n",
      "\n",
      "def pad_tensor(inp, pad, value=0):\n",
      "name   : pad_tensor\n",
      "params : ['inp', 'pad', 'value=0']\n",
      "comment: Util function for padding inp tensor\n",
      "\n",
      "class Reshape(Module):\n",
      "name   : Reshape(Module)\n",
      "params : ['shape']\n",
      "comment: Reshape layer\n",
      "\n",
      "class Flatten(Module):\n",
      "name   : Flatten(Module)\n",
      "params : []\n",
      "comment: Flatten layer\n",
      "\n",
      "class Conv(Module):\n",
      "name   : Conv(Module)\n",
      "params : ['c_in', 'c_out', 'k_s=3', 'stride=1', 'pad=0', 'leak=1.']\n",
      "comment: Convolutional layer\n",
      "\n",
      "def get_conv_model(data_bunch):\n",
      "name   : get_conv_model\n",
      "params : ['data_bunch']\n",
      "comment: Util function to get convolutional model based on data bunch shape\n",
      "\n",
      "class Module():\n",
      "name   : Module()\n",
      "params : []\n",
      "comment: Similar to pytorch Module, parent class to layers\n",
      "\n",
      "class Linear(Module):\n",
      "name   : Linear(Module)\n",
      "params : ['in_dim', 'num_hidden', 'end=False', 'require_grad=True']\n",
      "comment: Linear layer\n",
      "\n",
      "class ReLU(Module):\n",
      "name   : ReLU(Module)\n",
      "params : []\n",
      "comment: ReLU activation function (as a module)\n",
      "\n",
      "class CrossEntropy(Module):\n",
      "name   : CrossEntropy(Module)\n",
      "params : []\n",
      "comment: Cross Entropy loss function (as a module)\n",
      "\n",
      "def get_lin_model(data_bunch, num_hidden=50):\n",
      "name   : get_lin_model\n",
      "params : ['data_bunch', 'num_hidden=50']\n",
      "comment: Util function for obtaining two (linear) layer fully connected model\n",
      "\n",
      "def get_image_list(transforms):\n",
      "name   : get_image_list\n",
      "params : ['transforms']\n",
      "comment: Util function for getting image list from path with optional transformation\n",
      "\n",
      "def show_image_sub(img, ax=None, figsize=(3, 3)):\n",
      "name   : show_image_sub\n",
      "params : ['img', 'ax=None', 'figsize=(3', '3)']\n",
      "comment: Util function for subplotting image in given figure\n",
      "\n",
      "def show_image_batch(batch, c=4, r=None, figsize=None):\n",
      "name   : show_image_batch\n",
      "params : ['batch', 'c=4', 'r=None', 'figsize=None']\n",
      "comment: Util function for plotting batch of images with customizable dimentions of subplot and figure size\n",
      "\n",
      "class PilTransform(Transform):\n",
      "name   : PilTransform(Transform)\n",
      "params : []\n",
      "comment: General PIL library transformation class\n",
      "\n",
      "class PilRandomAllRotations(PilTransform):\n",
      "name   : PilRandomAllRotations(PilTransform)\n",
      "params : ['p=0.75']\n",
      "comment: Random rotation transformation class\n",
      "\n",
      "def process_size(size):\n",
      "name   : process_size\n",
      "params : ['size']\n",
      "comment: Util function to ensure size is two dimensional tuple\n",
      "\n",
      "def default_crop_size(w, h):\n",
      "name   : default_crop_size\n",
      "params : ['w', 'h']\n",
      "comment: Util function to obtain crop size from given width and height\n",
      "\n",
      "class GeneralCrop(PilTransform):\n",
      "name   : GeneralCrop(PilTransform)\n",
      "params : ['size', 'crop_size=None', 'resample=PIL.Image.BILINEAR']\n",
      "comment: General crop transformation class\n",
      "\n",
      "class CenterCrop(GeneralCrop):\n",
      "name   : CenterCrop(GeneralCrop)\n",
      "params : ['size', 'scale=1.14', 'resample=PIL.Image.BILINEAR']\n",
      "comment: Center crop transformation class\n",
      "\n",
      "class RandomResizedCrop(GeneralCrop):\n",
      "name   : RandomResizedCrop(GeneralCrop)\n",
      "params : ['size', 'scale=(0.08', '1.0)', 'ratio=(3./4.', '4./3.)', 'resample=PIL.Image.BILINEAR']\n",
      "comment: Randomized crop transformation class (common used on ImageNet data)\n",
      "\n",
      "def find_coeffs(src, tar):\n",
      "name   : find_coeffs\n",
      "params : ['src', 'tar']\n",
      "comment: Util function for obtaining warp coefficients with source and target coordinates\n",
      "\n",
      "def warp(img, size, src, resample=PIL.Image.BILINEAR):\n",
      "name   : warp\n",
      "params : ['img', 'size', 'src', 'resample=PIL.Image.BILINEAR']\n",
      "comment: Warp image by source coordinates and size\n",
      "\n",
      "def uniform(a, b):\n",
      "name   : uniform\n",
      "params : ['a', 'b']\n",
      "comment: Util function to sample from from uniform distribution\n",
      "\n",
      "class WarpRandomCrop(PilTransform):\n",
      "name   : WarpRandomCrop(PilTransform)\n",
      "params : ['size', 'crop_size=None', 'magnitude=0.', 'resample=PIL.Image.BILINEAR']\n",
      "comment: Random Warp transformation class\n",
      "\n",
      "class MaxPool(Module):\n",
      "name   : MaxPool(Module)\n",
      "params : ['k_s=3', 'stride=1', 'pad=0']\n",
      "comment: Max Pooling layer\n",
      "\n",
      "class AvgPool(Module):\n",
      "name   : AvgPool(Module)\n",
      "params : ['k_s=3', 'stride=1', 'pad=0']\n",
      "comment: Average Pooling layer\n",
      "\n",
      "def get_conv_pool_model(data_bunch):\n",
      "name   : get_conv_pool_model\n",
      "params : ['data_bunch']\n",
      "comment: Util function to get convolution model with average pooling\n",
      "\n",
      "def momentum_step(param, learning_rate, avg_grad, **kwargs):\n",
      "name   : momentum_step\n",
      "params : ['param', 'learning_rate', 'avg_grad', '**kwargs']\n",
      "comment: Momentum Stepping Function\n",
      "\n",
      "class StatefulOpt():\n",
      "name   : StatefulOpt()\n",
      "params : ['params', 'steppers=None', 'stats=None', '**hyper_params']\n",
      "comment: Improved StatelessOpt by allowing past hyperparameters values to be stored in states\n",
      "\n",
      "class Stat():\n",
      "name   : Stat()\n",
      "params : []\n",
      "comment: Class for keeping track of measurement\n",
      "\n",
      "class WeightedSumGrad(Stat):\n",
      "name   : WeightedSumGrad(Stat)\n",
      "params : []\n",
      "comment: Weighted gradient measurement\n",
      "\n",
      "class StepCount(Stat):\n",
      "name   : StepCount(Stat)\n",
      "params : []\n",
      "comment: Simple measurement to keep track of how many updates were done\n",
      "\n",
      "class ExpWeightedGrad(Stat):\n",
      "name   : ExpWeightedGrad(Stat)\n",
      "params : ['dampening=False']\n",
      "comment: Exponentially weighted moving avg of gradient\n",
      "\n",
      "class ExpWeightedSqrGrad(Stat):\n",
      "name   : ExpWeightedSqrGrad(Stat)\n",
      "params : ['dampening=True']\n",
      "comment: Exponentially weighted moving avg of squared gradient\n",
      "\n",
      "def debias(mom, damp, step):\n",
      "name   : debias\n",
      "params : ['mom', 'damp', 'step']\n",
      "comment: Util function to compute the debias coefficient for adam optimizer\n",
      "\n",
      "def adam(param, learning_rate, mom, damp_mom, step, sqr_mom, sqr_damp_mom, avg_grad, sqr_avg_grad, eps=1e-5, **kwargs):\n",
      "name   : adam\n",
      "params : ['param', 'learning_rate', 'mom', 'damp_mom', 'step', 'sqr_mom', 'sqr_damp_mom', 'avg_grad', 'sqr_avg_grad', 'eps=1e-5', '**kwargs']\n",
      "comment: Adam optimizer stepper\n",
      "\n",
      "def adam_opt(model, beta1=0.9, beta2=0.99, **kwargs):\n",
      "name   : adam_opt\n",
      "params : ['model', 'beta1=0.9', 'beta2=0.99', '**kwargs']\n",
      "comment: Util function to get adam optimizer\n",
      "\n",
      "def lamb_step(param, learning_rate, mom, damp_mom, step, sqr_mom, sqr_damp_mom, avg_grad, sqr_avg_grad, weight_decay, eps=1e-5, **kwargs):\n",
      "name   : lamb_step\n",
      "params : ['param', 'learning_rate', 'mom', 'damp_mom', 'step', 'sqr_mom', 'sqr_damp_mom', 'avg_grad', 'sqr_avg_grad', 'weight_decay', 'eps=1e-5', '**kwargs']\n",
      "comment: LAMB optimizer stepper\n",
      "\n",
      "def lamb_opt(model, beta1=0.9, beta2=0.99, **kwargs):\n",
      "name   : lamb_opt\n",
      "params : ['model', 'beta1=0.9', 'beta2=0.99', '**kwargs']\n",
      "comment: Util function to get LAMB optimizer\n",
      "\n",
      "class Sequential():\n",
      "name   : Sequential()\n",
      "params : ['*args']\n",
      "comment: Sequential Model with stored layers and training status\n",
      "\n",
      "def init_weight_he(d1, d2):\n",
      "name   : init_weight_he\n",
      "params : ['d1', 'd2']\n",
      "comment: He init for linear layer weight\n",
      "\n",
      "def init_weight_norm(d1, d2):\n",
      "name   : init_weight_norm\n",
      "params : ['d1', 'd2']\n",
      "comment: init weight with N(0, 1) distribution (not suitable for relu activation)\n",
      "\n",
      "def init_bias_zero(d):\n",
      "name   : init_bias_zero\n",
      "params : ['d']\n",
      "comment: init bias with zeros\n",
      "\n",
      "def init_bias_uni(d):\n",
      "name   : init_bias_uni\n",
      "params : ['d']\n",
      "comment: init bias with N(0, 1) distribution\n",
      "\n",
      "def init_weight(d1, d2, end=False):\n",
      "name   : init_weight\n",
      "params : ['d1', 'd2', 'end=False']\n",
      "comment: initialize linear layer weight based on whether it is follwed by ReLU activation\n",
      "\n",
      "def init_bias(d, zero=True):\n",
      "name   : init_bias\n",
      "params : ['d', 'zero=True']\n",
      "comment: initialize linear layer bias, default to 0 initialization\n",
      "\n",
      "def init_2d_weight(shape, leak=1.):\n",
      "name   : init_2d_weight\n",
      "params : ['shape', 'leak=1.']\n",
      "comment: init 2d weight\n",
      "\n",
      "def init_4d_weight(shape, leak=1.):\n",
      "name   : init_4d_weight\n",
      "params : ['shape', 'leak=1.']\n",
      "comment: init 4d weight\n",
      "\n",
      "def softmax(inp):\n",
      "name   : softmax\n",
      "params : ['inp']\n",
      "comment: naive softmax fn. Prone to large floating point errors\n",
      "\n",
      "def log_sum_exp(inp):\n",
      "name   : log_sum_exp\n",
      "params : ['inp']\n",
      "comment: LogSumExp trick (https://en.wikipedia.org/wiki/LogSumExp)\n",
      "\n",
      "def log_softmax(inp):\n",
      "name   : log_softmax\n",
      "params : ['inp']\n",
      "comment: log softmax fn using logSumExp trick to avoid large floating point errors\n",
      "\n",
      "def nll_loss(pre, tar):\n",
      "name   : nll_loss\n",
      "params : ['pre', 'tar']\n",
      "comment: Negative log-likelihood\n",
      "\n",
      "def cross_entropy(pre, tar):\n",
      "name   : cross_entropy\n",
      "params : ['pre', 'tar']\n",
      "comment: Cross entropy loss\n",
      "\n",
      "def compute_accuracy(pre, tar):\n",
      "name   : compute_accuracy\n",
      "params : ['pre', 'tar']\n",
      "comment: Accuracy\n",
      "\n",
      "class ResLayer(Module):\n",
      "name   : ResLayer(Module)\n",
      "params : ['i', 'o', 's', 'bottleneck']\n",
      "comment: Get ResLayer (almost a ResBlock but not including the final activation)\n",
      "\n",
      "class ResBlock(SubModel):\n",
      "name   : ResBlock(SubModel)\n",
      "params : ['i', 'o', 's', 'bottleneck']\n",
      "comment: ResBlock (ResLayer + Activation)\n",
      "\n",
      "def process_mnist(xt, yt, xv, yv):\n",
      "name   : process_mnist\n",
      "params : ['xt', 'yt', 'xv', 'yv']\n",
      "comment: Process mnist data with normalization\n",
      "\n",
      "def get_mnist_data():\n",
      "name   : get_mnist_data\n",
      "params : ['']\n",
      "comment: Get mnist dataset\n",
      "\n",
      "def get_data(name):\n",
      "name   : get_data\n",
      "params : ['name']\n",
      "comment: Get dataset by name\n",
      "\n",
      "def show_random_image(imgs):\n",
      "name   : show_random_image\n",
      "params : ['imgs']\n",
      "comment: Show random image from a batch with matplotlib\n",
      "\n",
      "def plot_by_epoch(data, label):\n",
      "name   : plot_by_epoch\n",
      "params : ['data', 'label']\n",
      "comment: Plot data by epoch and label data with given name\n",
      "\n",
      "def test(a, b, cmp, cname=None):\n",
      "name   : test\n",
      "params : ['a', 'b', 'cmp', 'cname=None']\n",
      "comment: General two value test with selectable comparison function\n",
      "\n",
      "def near(a, b):\n",
      "name   : near\n",
      "params : ['a', 'b']\n",
      "comment: Function to assert tensor similarity\n",
      "\n",
      "def test_near(a, b):\n",
      "name   : test_near\n",
      "params : ['a', 'b']\n",
      "comment: Test whether two tensors are near each other to account for floating point errors\n",
      "\n",
      "def test_near_zero(a, tol=1e-3):\n",
      "name   : test_near_zero\n",
      "params : ['a', 'tol=1e-3']\n",
      "comment: Test whether tensor is near the zero tensor of it's size\n",
      "\n",
      "def test_eq(a, b):\n",
      "name   : test_eq\n",
      "params : ['a', 'b']\n",
      "comment: Test whether two tensors are exactly equal\n",
      "\n",
      "def matmul_naive(a, b):\n",
      "name   : matmul_naive\n",
      "params : ['a', 'b']\n",
      "comment: Brute force matmul with 3 levels of for loop\n",
      "\n",
      "def matmul_element(a, b):\n",
      "name   : matmul_element\n",
      "params : ['a', 'b']\n",
      "comment: Sped up matmul_naive by dot producting 1d vectors\n",
      "\n",
      "def matmul_broadcast(a, b):\n",
      "name   : matmul_broadcast\n",
      "params : ['a', 'b']\n",
      "comment: Sped up matmul_element by leveraging pytorch tensor broadcasting\n",
      "\n",
      "def matmul_einsum(a, b):\n",
      "name   : matmul_einsum\n",
      "params : ['a', 'b']\n",
      "comment: Sped up matmul_broadcast by using pytorch built-in function (einstein sum notation)\n",
      "\n",
      "def matmul_torch(a, b):\n",
      "name   : matmul_torch\n",
      "params : ['a', 'b']\n",
      "comment: Sped up matmul_broadcast by using pytorch built-in function (@ operator)\n",
      "\n",
      "def normalize(x, m=None, s=None):\n",
      "name   : normalize\n",
      "params : ['x', 'm=None', 's=None']\n",
      "comment: Normalize input x with optional mean and std\n",
      "\n",
      "def TConvNReLU(i, o, norm, k=3, s=2, b=True):\n",
      "name   : TConvNReLU\n",
      "params : ['i', 'o', 'norm', 'k=3', 's=2', 'b=True']\n",
      "comment: Tranpose convolutional layer + normalization layer + ReLU activation\n",
      "\n",
      "def PadConvNReLU(i, o, norm, pad_mode, k=3, s=1, p=1, b=True, act=True, init=nn.init.kaiming_normal_):\n",
      "name   : PadConvNReLU\n",
      "params : ['i', 'o', 'norm', 'pad_mode', 'k=3', 's=1', 'p=1', 'b=True', 'act=True', 'init=nn.init.kaiming_normal_']\n",
      "comment: PaddedConvolutional layer + normalization layer + ReLU activation\n",
      "\n",
      "class ResnetBlock(nn.Module):\n",
      "name   : ResnetBlock(nn.Module)\n",
      "params : ['d', 'pad_mode', 'norm=None', 'dropout=0.', 'b=True']\n",
      "comment: ResBlock ( y = F(x) + x)\n",
      "\n",
      "def generator(i, o, c=64, norm=None, dropout=0., depth=6, pad_mode='reflection'):\n",
      "name   : generator\n",
      "params : ['i', 'o', 'c=64', 'norm=None', 'dropout=0.', 'depth=6', \"pad_mode='reflection'\"]\n",
      "comment: Generator of GAN\n",
      "\n",
      "def ConvNLReLU(i, o, norm, k=3, s=1, p=1, b=True, act=True, slope=0.2, init=nn.init.kaiming_normal_):\n",
      "name   : ConvNLReLU\n",
      "params : ['i', 'o', 'norm', 'k=3', 's=1', 'p=1', 'b=True', 'act=True', 'slope=0.2', 'init=nn.init.kaiming_normal_']\n",
      "comment: Convolutional layer + normalization layer + ReLU activation\n",
      "\n",
      "def discriminator(i, c=64, norm=None, n_layer=3, act=False):\n",
      "name   : discriminator\n",
      "params : ['i', 'c=64', 'norm=None', 'n_layer=3', 'act=False']\n",
      "comment: Discriminator of GAN\n",
      "\n",
      "class CycleGAN(nn.Module):\n",
      "name   : CycleGAN(nn.Module)\n",
      "params : ['i', 'o', 'c=64', 'norm=None', 'gen_layers=6', 'dis_layers=3', 'dropout=0.', 'lsgan=True']\n",
      "comment: Cycle GAN with two generators and discriminators\n",
      "\n",
      "class AdaptiveLoss(nn.Module):\n",
      "name   : AdaptiveLoss(nn.Module)\n",
      "params : ['critic']\n",
      "comment: Critic wrapper for creating binary target tensors for loss computation\n",
      "\n",
      "class CycleGANLoss(nn.Module):\n",
      "name   : CycleGANLoss(nn.Module)\n",
      "params : ['cycleGAN', 'w1=10.', 'w2=10.', 'wi=0.5', 'lsgan=True']\n",
      "comment: Cycle GAN loss layer with identity loss, generator loss, and discriminator loss\n",
      "\n",
      "class CycleGANTrainer(LearnerCallback):\n",
      "name   : CycleGANTrainer(LearnerCallback)\n",
      "params : []\n",
      "comment: Callback for the special training procedure of Cycle GAN\n",
      "\n",
      "class PositionalEncoding(nn.Module):\n",
      "name   : PositionalEncoding(nn.Module)\n",
      "params : ['d']\n",
      "comment: Positional encoding layer using sinusoid curve\n",
      "\n",
      "class TransformerEmbedding(nn.Module):\n",
      "name   : TransformerEmbedding(nn.Module)\n",
      "params : ['vocab_size', 'emb_size', 'dropout=0.']\n",
      "comment: Transformer embedding layer (embedding + positional encoding + dropout)\n",
      "\n",
      "def feed_forward(model_dim, inner_dim, drop_ff=0., double_drop=True):\n",
      "name   : feed_forward\n",
      "params : ['model_dim', 'inner_dim', 'drop_ff=0.', 'double_drop=True']\n",
      "comment: Feed forward layer after multi-headed attention (2 * Linear + skip connection + layer norm)\n",
      "\n",
      "class MultiHeadAttention(nn.Module):\n",
      "name   : MultiHeadAttention(nn.Module)\n",
      "params : ['num_head', 'model_dim', 'head_dim=None', 'drop_res=0.', 'drop_att=0.', 'bias=True', 'scale=True']\n",
      "comment: Multi-Head Attention with key, query, value\n",
      "\n",
      "class TransEncoder(nn.Module):\n",
      "name   : TransEncoder(nn.Module)\n",
      "params : ['num_head', 'model_dim', 'head_dim', 'inner_dim', 'drop_res=0.', 'drop_att=0.', 'drop_ff=0.', 'bias=True', 'scale=True', 'double_drop=True']\n",
      "comment: Encoder block of a Transformer model\n",
      "\n",
      "class TransDecoder(nn.Module):\n",
      "name   : TransDecoder(nn.Module)\n",
      "params : ['num_head', 'model_dim', 'head_dim', 'inner_dim', 'drop_res=0.', 'drop_att=0.', 'drop_ff=0.', 'bias=True', 'scale=True', 'double_drop=True']\n",
      "comment: Decoder block of a Transformer model\n",
      "\n",
      "class Transformer(nn.Module):\n",
      "name   : Transformer(nn.Module)\n",
      "params : ['vocab_size', 'out_size', 'n_layers=6', 'num_head=8', 'model_dim=256', 'head_dim=32', 'inner_dim=1024', 'drop_inp=0.1', 'drop_res=0.1', 'drop_att=0.1', 'drop_ff=0.1', 'bias=True', 'scale=True', 'double_drop=True', 'pad_idx=1']\n",
      "comment: Transformer model\n",
      "\n",
      "class SubModel(Module):\n",
      "name   : SubModel(Module)\n",
      "params : []\n",
      "comment: Wrapper class around model to allow models to branch off into sub models (used in ResNet)\n",
      "\n",
      "class Optimizer():\n",
      "name   : Optimizer()\n",
      "params : ['parameters', 'learning_rate']\n",
      "comment: Vanilla optimizer with basic methods\n",
      "\n",
      "class DynamicOpt():\n",
      "name   : DynamicOpt()\n",
      "params : ['parameters', '**hyper_params']\n",
      "comment: Dynamic optimizer to allow multiple hyper parameters and param scheduling\n",
      "\n",
      "class ProgressViewer(Callback):\n",
      "name   : ProgressViewer(Callback)\n",
      "params : []\n",
      "comment: Callback utilizing FastAI frontend lib to display neat looking training progress\n",
      "\n",
      "def listify(inp):\n",
      "name   : listify\n",
      "params : ['inp']\n",
      "comment: Convert input of different types to a list\n",
      "\n",
      "def setify(inp):\n",
      "name   : setify\n",
      "params : ['inp']\n",
      "comment: Convert input of different types to a set\n",
      "\n",
      "def uniqueify(items, sort=False):\n",
      "name   : uniqueify\n",
      "params : ['items', 'sort=False']\n",
      "comment: Convert input of different types to a unique, optionally sorted list with no duplicates\n",
      "\n",
      "def get_files(path, exts=None, recurse=False, include=None):\n",
      "name   : get_files\n",
      "params : ['path', 'exts=None', 'recurse=False', 'include=None']\n",
      "comment: Get files of wanted extensions in specified path\n",
      "\n",
      "class ListContainer():\n",
      "name   : ListContainer()\n",
      "params : ['items):self.items=listify(items']\n",
      "comment: Imitated numpy list with multi-index select and boolean masking\n",
      "\n",
      "def compose(item, fns, *args, order_key='_order', **kwargs):\n",
      "name   : compose\n",
      "params : ['item', 'fns', '*args', \"order_key='_order'\", '**kwargs']\n",
      "comment: Apply multiple functions on input items in predefined order\n",
      "\n",
      "class ItemList(ListContainer):\n",
      "name   : ItemList(ListContainer)\n",
      "params : ['items', \"path='.'\", 'transforms=None']\n",
      "comment: List container with option to apply transformation to data\n",
      "\n",
      "class ImageList(ItemList):\n",
      "name   : ImageList(ItemList)\n",
      "params : []\n",
      "comment: General transformation class\n",
      "\n",
      "def to_byte_tensor(item):\n",
      "name   : to_byte_tensor\n",
      "params : ['item']\n",
      "comment: Item to byte tensor transformation class\n",
      "\n",
      "def to_float_tensor(item):\n",
      "name   : to_float_tensor\n",
      "params : ['item']\n",
      "comment: Item to float tensor transformation class (expect item to be byte tensor)\n",
      "\n",
      "def make_rgb(item):\n",
      "name   : make_rgb\n",
      "params : ['item']\n",
      "comment: Item to RGB image transformation class\n",
      "\n",
      "def split_grandparent(filepath, train_name='train', valid_name='valid'):\n",
      "name   : split_grandparent\n",
      "params : ['filepath', \"train_name='train'\", \"valid_name='valid'\"]\n",
      "comment: Determine data type by grandparent name\n",
      "\n",
      "def split_by_fn(item_list, fn):\n",
      "name   : split_by_fn\n",
      "params : ['item_list', 'fn']\n",
      "comment: Split item list with custom function output (expected to be binary) \n",
      "\n",
      "class SplitData():\n",
      "name   : SplitData()\n",
      "params : ['train', 'valid']\n",
      "comment: Splitted Data class with train and valid data\n",
      "\n",
      "class Processor():\n",
      "name   : Processor()\n",
      "params : []\n",
      "comment: General data processor class\n",
      "\n",
      "class CategoryProcessor(Processor):\n",
      "name   : CategoryProcessor(Processor)\n",
      "params : []\n",
      "comment: Categorical data processor to labels\n",
      "\n",
      "def p_name(filepath):\n",
      "name   : p_name\n",
      "params : ['filepath']\n",
      "comment: Util function for file parent name\n",
      "\n",
      "def gp_name(filepath):\n",
      "name   : gp_name\n",
      "params : ['filepath']\n",
      "comment: Util function for file grandparent name\n",
      "\n",
      "class LabeledData():\n",
      "name   : LabeledData()\n",
      "params : ['x', 'y', 'proc_x', 'proc_y']\n",
      "comment: Labeled Data class with data and processed data\n",
      "\n",
      "def label_by_fn(splitted_data, fn, proc_x=None, proc_y=None):\n",
      "name   : label_by_fn\n",
      "params : ['splitted_data', 'fn', 'proc_x=None', 'proc_y=None']\n",
      "comment: Util function for labelling both train and valid data\n",
      "\n",
      "def show_image(img, figsize=(3, 3)):\n",
      "name   : show_image\n",
      "params : ['img', 'figsize=(3', '3)']\n",
      "comment: Show input image with matplotlib\n",
      "\n",
      "class ItersStopper(Callback):\n",
      "name   : ItersStopper(Callback)\n",
      "params : ['end_iter=10']\n",
      "comment: Callback to stop on specified batch/iteration (good for debugging training loop)\n",
      "\n",
      "class EpochsStopper(Callback):\n",
      "name   : EpochsStopper(Callback)\n",
      "params : ['end_epoch=10']\n",
      "comment: Callback for stopping at specified epoch\n",
      "\n",
      "class AccuracyStopper(Callback):\n",
      "name   : AccuracyStopper(Callback)\n",
      "params : ['patience=5', 'verbose=True']\n",
      "comment: Callback for stopping training after model does not receive improvement in specific number of epochs\n",
      "\n",
      "class AvgStats():\n",
      "name   : AvgStats()\n",
      "params : ['metrics', 'training']\n",
      "comment: Class for keeping track of average loss and other methods\n",
      "\n",
      "class StatsLogging(Callback):\n",
      "name   : StatsLogging(Callback)\n",
      "params : ['metrics=[compute_accuracy]']\n",
      "comment: Callback for print out loss and other metrics for each training epoch\n",
      "\n",
      "class LSTMCell(nn.Module):\n",
      "name   : LSTMCell(nn.Module)\n",
      "params : ['i_dim', 'h_dim']\n",
      "comment: LSTM cell (naive implementation)\n",
      "\n",
      "class LSTMLayer(nn.Module):\n",
      "name   : LSTMLayer(nn.Module)\n",
      "params : ['i_dim', 'h_dim']\n",
      "comment: Wrapper for passing different input timestamps into LSTM cell\n",
      "\n",
      "class FastLSTMCell(nn.Module):\n",
      "name   : FastLSTMCell(nn.Module)\n",
      "params : ['i_dim', 'h_dim']\n",
      "comment: LSTM cell (fast implementation using linear layers)\n",
      "\n",
      "class FastLSTMLayer(nn.Module):\n",
      "name   : FastLSTMLayer(nn.Module)\n",
      "params : []\n",
      "comment: Wrapper for passing different input timestamps into FastLSTM cell\n",
      "\n",
      "class BERTEmbLayer(nn.Module):\n",
      "name   : BERTEmbLayer(nn.Module)\n",
      "params : ['vocab_size', 'emb_size', 'seq_len']\n",
      "comment: BERT embedding layer (position + token + segment)\n",
      "\n",
      "class BERT(nn.Module):\n",
      "name   : BERT(nn.Module)\n",
      "params : ['vocab_size', 'emb_size', 'seq_len', 'n_layers=6', 'num_head=8', 'model_dim=256', 'head_dim=32', 'inner_dim=1024', 'drop_res=0.1', 'drop_att=0.1', 'drop_ff=0.1', 'bias=True', 'scale=True', 'double_drop=True']\n",
      "comment: BERT model\n",
      "\n",
      "def sgd(param, learning_rate, **kwargs):\n",
      "name   : sgd\n",
      "params : ['param', 'learning_rate', '**kwargs']\n",
      "comment: Basic stochastic gradient descent\n",
      "\n",
      "def l2_reg(param, weight_decay, **kwargs):\n",
      "name   : l2_reg\n",
      "params : ['param', 'weight_decay', '**kwargs']\n",
      "comment: L2 regularization\n",
      "\n",
      "def compose_inplace(item, fns, **hyper_params):\n",
      "name   : compose_inplace\n",
      "params : ['item', 'fns', '**hyper_params']\n",
      "comment: Simplified compose function that modifies input item in-place\n",
      "\n",
      "class StatelessOpt():\n",
      "name   : StatelessOpt()\n",
      "params : ['params', 'steppers=None', '**hyper_params']\n",
      "comment: Improve dynamicOpt that allows different layers to have different hyperparameters\n",
      "\n",
      "class Recorder(Callback):\n",
      "name   : Recorder(Callback)\n",
      "params : [\"param_names=['learning_rate']\"]\n",
      "comment: Callback for recording specified hyper parameters (good for debugging param scheduler)\n",
      "\n",
      "class LearningRateSearch(Callback):\n",
      "name   : LearningRateSearch(Callback)\n",
      "params : ['max_iter=1000', 'min_lr=1e-4', 'max_lr=1']\n",
      "comment: Callback to search for optimal learning rate before actual training\n",
      "\n",
      "class ParamScheduler(Callback):\n",
      "name   : ParamScheduler(Callback)\n",
      "params : ['param_name', 'schedule_fn']\n",
      "comment: Callback for scheduling hyper parameter value each epoch\n",
      "\n",
      "class GRUCell(nn.Module):\n",
      "name   : GRUCell(nn.Module)\n",
      "params : ['i_dim', 'h_dim']\n",
      "comment: GRU cell\n",
      "\n",
      "class GRULayer(nn.Module):\n",
      "name   : GRULayer(nn.Module)\n",
      "params : ['i_dim', 'h_dim']\n",
      "comment: Wrapper for passing different input timestamps into GRU cell\n",
      "\n",
      "def weighted_sum(t1, t2, ratio):\n",
      "name   : weighted_sum\n",
      "params : ['t1', 't2', 'ratio']\n",
      "comment: Util function for linear combination of two elements\n",
      "\n",
      "class BatchNorm(Module):\n",
      "name   : BatchNorm(Module)\n",
      "params : ['c', 'momentum=0.1', 'epsilon=1e-6']\n",
      "comment: Batch normalization layer\n",
      "\n",
      "def get_conv_final_model(data_bunch):\n",
      "name   : get_conv_final_model\n",
      "params : ['data_bunch']\n",
      "comment: Util function to get convolutional model with pooling and batch normalization layers\n",
      "\n",
      "class Parameter():\n",
      "name   : Parameter()\n",
      "params : ['data', 'requires_grad=True']\n",
      "comment: Model parameter class with tensor data and gradient to imitate a basic pytorch tensor\n",
      "\n",
      "class Identity(Module):\n",
      "name   : Identity(Module)\n",
      "params : []\n",
      "comment: Identity layer (for skip connections in ResNet)\n",
      "\n",
      "class LearningRateSearch(Callback):\n",
      "name   : LearningRateSearch(Callback)\n",
      "params : ['max_iter=1000', 'min_lr=1e-4', 'max_lr=1']\n",
      "comment: Callback to search for optimal learning rate before actual training\n",
      "\n",
      "def plot_lr_loss(LearningRateSearch):\n",
      "name   : plot_lr_loss\n",
      "params : ['LearningRateSearch']\n",
      "comment: Util function for plotting relationship of loss vs. learning rate\n",
      "\n",
      "def annealer(fn):\n",
      "name   : annealer\n",
      "params : ['fn']\n",
      "comment: Decorator function to produce partial function of param schedule\n",
      "\n",
      "def schedule_lin(start, end, position):\n",
      "name   : schedule_lin\n",
      "params : ['start', 'end', 'position']\n",
      "comment: Linear param schedule\n",
      "\n",
      "def schedule_cos(start, end, position):\n",
      "name   : schedule_cos\n",
      "params : ['start', 'end', 'position']\n",
      "comment: Cosine param schedule\n",
      "\n",
      "def schedule_none(start, end, position):\n",
      "name   : schedule_none\n",
      "params : ['start', 'end', 'position']\n",
      "comment: No param schedule (param values stay at start)\n",
      "\n",
      "def schedule_exp(start, end, position):\n",
      "name   : schedule_exp\n",
      "params : ['start', 'end', 'position']\n",
      "comment: Exponential param schedule\n",
      "\n",
      "def combine_schedules(segments, ranges):\n",
      "name   : combine_schedules\n",
      "params : ['segments', 'ranges']\n",
      "comment: Combine multiple schedules with they respective segments and ranges\n",
      "\n",
      "def plot_schedule(schedule):\n",
      "name   : plot_schedule\n",
      "params : ['schedule']\n",
      "comment: Util function for plotting param schedule\n",
      "\n",
      "def one_cycle_cos(start, upper, end):\n",
      "name   : one_cycle_cos\n",
      "params : ['start', 'upper', 'end']\n",
      "comment: Wrapper function to create 1-cycle-training param schedule\n",
      "\n",
      "class Recorder(Callback):\n",
      "name   : Recorder(Callback)\n",
      "params : [\"param_names=['learning_rate']\"]\n",
      "comment: Callback for recording specified hyper parameters (good for debugging param scheduler)\n",
      "\n",
      "class ParamScheduler(Callback):\n",
      "name   : ParamScheduler(Callback)\n",
      "params : ['param_name', 'schedule_fn']\n",
      "comment: Callback for scheduling hyper parameter value each epoch\n",
      "\n",
      "def fit(num_epochs, data_bunch, model, loss_fn, optimizer):\n",
      "name   : fit\n",
      "params : ['num_epochs', 'data_bunch', 'model', 'loss_fn', 'optimizer']\n",
      "comment: Basic training loop\n",
      "\n",
      "class EpochLogger(Callback):\n",
      "name   : EpochLogger(Callback)\n",
      "params : []\n",
      "comment: Most simple callback to just log epoch\n",
      "\n",
      "class CancelTrainException(Exception):\n",
      "name   : CancelTrainException(Exception)\n",
      "params : []\n",
      "comment: Exception class for early stopping training\n",
      "\n",
      "class CancelBatchException(Exception):\n",
      "name   : CancelBatchException(Exception)\n",
      "params : []\n",
      "comment: Exception class for early stopping batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean()\n",
    "htmls = scripts2htmls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
