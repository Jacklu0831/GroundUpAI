# ---------------------------------------------
# | THIS FILE WAS AUTOGENERATED! DO NOT EDIT! |
# ---------------------------------------------
# edit notebooks/14_optimizer.ipynb and run generate_all.py

import sys
sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))

from sub_model import *

class Optimizer():
    def __init__(self, parameters, learning_rate):
        '''Vanilla optimizer with basic methods.
            parameters: model parameters
            learning_rate: step size of each training iteration
        '''
        self.parameters, self.learning_rate = parameters, learning_rate

    def step(self):
        for parameter in self.parameters:
            parameter.step(self.learning_rate)

    def zero_grad(self):
        for parameter in self.parameters:
            parameter.zero_grad()

    def __repr__(self):
        return f'(Optimizer) learning_rate: {self.learning_rate}'

class DynamicOpt():
    def __init__(self, parameters, **hyper_params):
        '''Dynamic optimizer to allow multiple hyper parameters and param scheduling.
            parameters: model parameters
            hyper_params: dictionary of hyper parameters optimizer keeps track of
        '''
        self.parameters = parameters
        self.hyper_params = dict(hyper_params)

    def step(self):
        for parameter in self.parameters:
            parameter.step(self.hyper_params['learning_rate'])

    def zero_grad(self):
        for parameter in self.parameters:
            parameter.zero_grad()

    def __repr__(self):
        return f'(DynamicOpt) hyper_params: {list(self.hyper_params)}'