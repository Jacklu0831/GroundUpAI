# ---------------------------------------------
# | THIS FILE WAS AUTOGENERATED! DO NOT EDIT! |
# ---------------------------------------------
# edit notebooks/Callbacks.ipynb and run generate_all.py

import sys
from os.path import join

sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))
from operations import *
from sequential_model import *
from training import *

import re

def camel2snake(name):
    camel_re1 = re.compile('(.)([A-Z][a-z]+)')
    camel_re2 = re.compile('([a-z0-9])([A-Z])')
    s1 = re.sub(camel_re1, r'\1_\2', name)
    return re.sub(camel_re2, r'\1_\2', s1).lower()

class Callback():
    order = 0
    def __getattr__(self, k):
        # delegate attribute checking to runner
        return getattr(self.runner, k)

    def set_runner(self, runner):
        self.runner = runner

    @property
    def name(self):
        return re.sub(r'Callback$', '', self.__class__.__name__) or 'callback'

    def __repr__(self):
        return f'(callback) {camel2snake(name)}'

class Runner():
    def __init__(self, learner, callback_fns=[]):
        self.learner = learner
        self.stop = False
        self.callbacks = [TrainEvalCallback()] + self.get_callbacks(callback_fns)

    def get_callbacks(self, callback_fns):
        callbacks = []
        for callback_fn in callback_fns:
            callback = callback_fn()
            setattr(self, callback.name, callback)
            callbacks.append(callback)
        return callbacks

    @property
    def data_bunch(self): return self.learner.data_bunch
    @property
    def model(self):      return self.learner.model
    @property
    def loss_fn(self):    return self.learner.loss_fn
    @property
    def optimizer(self):  return self.learner.optimizer

    def one_batch(self, x_batch, y_batch):
        self.x_batch = x_batch
        self.y_batch = y_batch

        if self('before_batch'): return
        self.pred = self.model(self.x_batch)
        if self('after_pred'): return
        self.loss = self.loss_fn(self.pred, self.y_batch)
        if self('after_loss'): return
        if not self.model.training: return
        self.loss.backward()
        if self('after_loss_back'): return
        self.model.backward()
        if self('after_model_back'): return
        self.opt.step()
        if self('after_step'): return
        self.opt.zero_grad()

    def all_batches(self):
        data_loader = self.data_bunch.train_ds if self.model.training else self.data_bunch.valid_ds
        self.iters_count, self.iters = 0, len(data_loader)
        for x_batch, y_batch in data_loader:
            if self.stop: break
            self.one_batch(x_batch, y_batch)
            self('after_batch')
        self.iters = 0
        self.stop = False

    def fit(self, num_epochs):
        self.num_epochs = num_epochs

        for callback in self.callbacks:
            callback.set_runner(self)

        if self('before_fit'): return

        for epoch in range(1, num_epochs+1):
            self.curr_epoch = epoch
            if self('begin_epoch'): return
            if self('before_train'): return
            self.all_batches()
            if self('before_valid'): return
            self.all_batches()
            if self('after_epoch'): break

        self('after_fit')

    def __call__(self, callback_name):
        for callback in sorted(self.callbacks, key=lambda x: x.order):
            fn = getattr(callback, callback_name, None) # default to None if not inherited (flexible)
            if fn and fn(): return True
        return False