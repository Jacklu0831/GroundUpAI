# ---------------------------------------------
# | THIS FILE WAS AUTOGENERATED! DO NOT EDIT! |
# ---------------------------------------------
# edit notebooks/Callbacks.ipynb and run generate_all.py

import sys
from os.path import join

sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))
from operations import *
from sequential_model import *
from training import *

import re

def camel2snake(name):
    camel_re1 = re.compile('(.)([A-Z][a-z]+)')
    camel_re2 = re.compile('([a-z0-9])([A-Z])')
    s1 = re.sub(camel_re1, r'\1_\2', name)
    return re.sub(camel_re2, r'\1_\2', s1).lower()

class Callback():
    order = 0
    def __getattr__(self, k):
        # delegate attribute checking to runner
        return getattr(self.runner, k)

    def set_runner(self, runner):
        self.runner = runner

    @property
    def name(self):
        return re.sub(r'Callback$', '', self.__class__.__name__) or 'callback'

    def __repr__(self):
        return f'(callback) {camel2snake(self.name)}'

class TrainEvalCallback(Callback):
    def before_train(self):
        self.model.train()

    def before_valid(self):
        self.model.eval_()

class ItersStopCallback(Callback):
    def after_batch(self):
        print(f'iteration: {self.iters_count}')
        if self.iters_count >= 10:
            self.runner.stop = True

class AvgStats():
    def __init__(self, metrics, training):
        self.metrics = metrics
        self.training = training

    def reset(self):
        self.count = 0
        self.total_loss = torch.Tensor([0])
        self.totals = [torch.Tensor([0])] * len(self.metrics)

    @property
    def all_stats(self): return [self.total_loss] + self.totals

    @property
    def avg_stats(self): return [s.item()/self.count for s in self.all_stats]

    def __repr__(self):
        if not self.count:
            return ''
        return f"{'train' if self.training else 'valid'} metrics - {self.avg_stats}"

    def accumulate(self, runner):
        batch_size = runner.x_batch.shape[0]
        self.count += batch_size
        self.total_loss = runner.loss * batch_size
        for i, metric in enumerate(self.metrics):
            self.totals[i] += metric(runner.pred, runner.y_batch) * batch_size

class StatsLoggingCallback(Callback):
    def __init__(self, metrics):
        self.train_stats = AvgStats(metrics, True)
        self.valid_stats = AvgStats(metrics, False)

    def before_epoch(self):
        self.train_stats.reset()
        self.valid_stats.reset()

    def after_loss(self):
        stats = self.train_stats if self.model.training else self.valid_stats
        stats.accumulate(self.runner)

    def after_epoch(self):
        print(f'Epoch - {self.epoch}\n{self.train_stats}\n{self.valid_stats}\n')

class Runner():
    def __init__(self, learner, callbacks=[]):
        self.learner = learner
        self.stop = False
        self.callbacks = sorted([TrainEvalCallback()] + callbacks, key=lambda cb: cb.order)
        for callback in self.callbacks:
            callback.runner = self

    def __repr__(self):
        return f'{self.learner}\n(Callbacks) ' + ' '.join(map(lambda cb: cb.name, self.callbacks))

    @property
    def data_bunch(self): return self.learner.data_bunch
    @property
    def model(self):      return self.learner.model
    @property
    def loss_fn(self):    return self.learner.loss_fn
    @property
    def optimizer(self):  return self.learner.optimizer

    def one_batch(self, x_batch, y_batch):
        self.x_batch = x_batch
        self.y_batch = y_batch

        if self('before_batch'):     return
        self.pred = self.model(self.x_batch)
        if self('after_pred'):       return
        self.loss = self.loss_fn(self.pred, self.y_batch)
        if self('after_loss'):       return
        if not self.model.training:  return
        self.loss_fn.backward()
        if self('after_loss_back'):  return
        self.model.backward()
        if self('after_model_back'): return
        self.optimizer.step()
        if self('after_step'):       return
        self.optimizer.zero_grad()

    def all_batches(self):
        data_loader = self.data_bunch.train_dl if self.model.training else self.data_bunch.valid_dl
        self.iters_count, self.iters = 0, len(data_loader)
        for x_batch, y_batch in data_loader:
            if self.stop: break
            self.one_batch(x_batch, y_batch)
            self.iters_count += 1
            self('after_batch')

    def fit(self, num_epochs):
        self.num_epochs = num_epochs

        for callback in self.callbacks:
            callback.set_runner(self)

        if self('before_fit'):       return
        for epoch in range(1, num_epochs+1):
            if self.stop: break
            self.epoch = epoch
            if self('before_epoch'): return
            if self('before_train'): return
            self.all_batches()
            if self('before_valid'): return
            self.all_batches()
            if self('after_epoch'): break
        self('after_fit')

    def __call__(self, callback_name):
        for callback in self.callbacks:
            fn = getattr(callback, callback_name, None) # default to None if not inherited (flexible)
            if fn and fn():
                return True
        return False