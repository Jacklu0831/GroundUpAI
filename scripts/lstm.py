# ---------------------------------------------
# | THIS FILE WAS AUTOGENERATED! DO NOT EDIT! |
# ---------------------------------------------
# edit notebooks/26_lstm.ipynb and run generate_all.py

import sys

sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))
from resnet import *

class LSTMCell(nn.Module):
    def __init__(self, i, h):
        '''LSTM cell (naive implementation).
            i: input data dimension
            h: number of hidden units in the lstm cell
        '''
        super().__init__()
        self.i, self.h = i, h
        self.Ui = nn.Parameter(init_2d_weight((i, h)))
        self.Uf = nn.Parameter(init_2d_weight((i, h)))
        self.Uo = nn.Parameter(init_2d_weight((i, h)))
        self.Ug = nn.Parameter(init_2d_weight((i, h)))
        self.Wi = nn.Parameter(init_2d_weight((h, h)))
        self.Wf = nn.Parameter(init_2d_weight((h, h)))
        self.Wo = nn.Parameter(init_2d_weight((h, h)))
        self.Wg = nn.Parameter(init_2d_weight((h, h)))

    def forward(self, x, state):
        h, c = state

        i   = (x @ self.Ui + h @ self.Wi).sigmoid()
        f   = (x @ self.Uf + h @ self.Wf).sigmoid()
        o   = (x @ self.Uo + h @ self.Wo).sigmoid()
        c_t = (x @ self.Ug + h @ self.Wg).tanh()

        c = (f*c + i*c_t).sigmoid()
        h = c.tanh() * o
        return h, (h, c)

    def __repr__(self):
        return f'LSTM({self.i}, {self.h})'

class LSTMLayer(nn.Module):
    def __init__(self, i, h):
        '''Wrapper for passing different input timestamps into LSTM cell
            i: input data dimension
            h: number of hidden units in the lstm cell
        '''
        super().__init__()
        self.cell = LSTMCell(i, h)

    def forward(self, inps, state):
        outputs = []
        for inp in inps.unbind(1):
            out, state = self.cell(inp, state)
            outputs.append(out)
        return torch.stack(outputs, 1), state

    def __repr__(self): return f'{self.cell}'

class FastLSTMCell(nn.Module):
    def __init__(self, i, h):
        '''LSTM cell (fast implementation using linear layers).
            i: input data dimension
            h: number of hidden units in the lstm cell
        '''
        super().__init__()
        self.i, self.h = i, h
        # also adds a small bias
        self.x_gates = nn.Linear(i, 4*h)
        self.h_gates = nn.Linear(i, 4*h)

    def forward(self, x, state):
        h, c = state
        gates = (self.x_gates(x) + self.h_gates(h)).chunk(4, 1)

        i   = gates[0].sigmoid()
        f   = gates[1].sigmoid()
        o   = gates[2].sigmoid()
        c_t = gates[3].tanh()

        c = f*c + i*c_t
        h = o * c.tanh()
        return h, (h, c)

    def __repr__(self): return f'LSTM({self.i}, {self.h})'

class FastLSTMLayer(nn.Module):
    def __init__(self, i, h):
        '''Wrapper for passing different input timestamps into FastLSTM cell
            i: input data dimension
            h: number of hidden units in the lstm cell
        '''
        super().__init__()
        self.cell = FastLSTMCell(i, h)

    def forward(self, inps, state):
        outputs = []
        for inp in inps.unbind(1):
            out, state = self.cell(inp, state)
            outputs.append(out)
        return torch.stack(outputs, 1), state

    def __repr__(self): return f'{self.cell}'