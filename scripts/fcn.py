# ---------------------------------------------
# | THIS FILE WAS AUTOGENERATED! DO NOT EDIT! |
# ---------------------------------------------
# edit notebooks/fcn.ipynb and run generate_all.py

import sys
import os
from os.path import join
import math

sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))
from operations import *

def he_init(m, n):
    return torch.randn(m, n) * (2./m)**0.5

def init(m, n, relu):
    if relu:
        return he_init(m, n)
    return torch.randn(m, n) * (1./m)**0.5

class Module():
    def __call__(self, *args):
        self.args = args
        self.out = self.fwd(*args)
        return self.out

    def forward(self):
        raise Exception('not implemented')

    def backward(self):
        self.bwd(self.out, *self.args)

class ReLU(Module):
    def fwd(self, inp):
        return inp.clamp_min(0.) - 0.5

    def bwd(self, out, inp):
        inp.g = (inp > 0).float() * out.g

class Lin(Module):
    def __init__(self, w, b):
        self.w = w
        self.b = b

    def fwd(self, inp):
        return inp @ self.w + self.b

    def bwd(self, out, inp):
        inp.g = out.g @ self.w.t()
        self.w.g = inp.t() @ out.g
        self.b.g = out.g.sum(0)

class Flatten(Module):
    def fwd(self,x):
        return x.view(-1)

    def bwd(self):
        pass

class Mse(Module):
    def fwd(self, inp, tar):
        return (inp.squeeze() - tar).pow(2).mean()

    def bwd(self, out, inp, tar):
        inp.g = 2 * (inp.squeeze() - tar).unsqueeze(-1) / tar.shape[0]

class Model():
    def __init__(self, layers):
        self.layers = layers
        self.loss = Mse()

    def __call__(self, x, tar):
        for layer in self.layers:
            x = layer(x)
        return x, self.loss(x, tar)

    def backward(self):
        self.loss.backward()
        for l in reversed(self.layers):
            l.backward()