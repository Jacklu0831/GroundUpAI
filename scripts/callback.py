# ---------------------------------------------
# | THIS FILE WAS AUTOGENERATED! DO NOT EDIT! |
# ---------------------------------------------
# edit notebooks/03_callback.ipynb and run generate_all.py

import sys
from os.path import join

sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))
from training import *

import re

def camel2snake(name):
    camel_re1 = re.compile('(.)([A-Z][a-z]+)')
    camel_re2 = re.compile('([a-z0-9])([A-Z])')
    s1 = re.sub(camel_re1, r'\1_\2', name)
    return re.sub(camel_re2, r'\1_\2', s1).lower()

class Callback():
    order = 0
    def __getattr__(self, k):
        # delegate attribute checking to runner
        return getattr(self.runner, k)

    def set_runner(self, runner):
        self.runner = runner

    @property
    def name(self):
        return re.sub(r'Callback$', '', self.__class__.__name__) or 'callback'

    def __repr__(self):
        return f'(callback) {camel2snake(self.name)}'

    def __call__(self, cb_name):
        fn = getattr(self, cb_name, None)
        if fn and fn():
            return True
        return False

class TrainEval(Callback):
    def before_train(self):
        self.model.train()

    def before_valid(self):
        self.model.eval_()

class ItersStop(Callback):
    def after_batch(self):
        print(f'iteration: {self.iters_count}')
        if self.iters_count >= 10:
            self.runner.stop = True

class Runner():
    def __init__(self, learner, callbacks=[]):
        self.learner = learner
        self.stop = False
        self.callbacks = sorted([TrainEval()] + callbacks, key=lambda cb: cb.order)
        for callback in self.callbacks:
            callback.runner = self

    def __repr__(self):
        return f'{self.learner}\n(Callbacks) ' + ' '.join(map(lambda cb: cb.name, self.callbacks))

    @property
    def data_bunch(self): return self.learner.data_bunch
    @property
    def model(self):      return self.learner.model
    @property
    def loss_fn(self):    return self.learner.loss_fn
    @property
    def optimizer(self):  return self.learner.optimizer

    def one_batch(self, x_batch, y_batch):
        self.x_batch = x_batch
        self.y_batch = y_batch

        if self('before_batch'):     return
        self.pred = self.model(self.x_batch)
        if self('after_pred'):       return
        self.loss = self.loss_fn(self.pred, self.y_batch)
        if self('after_loss'):       return
        if not self.model.training:  return
        self.loss_fn.backward()
        if self('after_loss_back'):  return
        self.model.backward()
        if self('after_model_back'): return
        self.optimizer.step()
        if self('after_step'):       return
        self.optimizer.zero_grad()

    def all_batches(self):
        data_loader = self.data_bunch.train_dl if self.model.training else self.data_bunch.valid_dl
        self.iters_count, self.iters = 0, len(data_loader)
        for x_batch, y_batch in data_loader:
            if self.stop: break
            self.one_batch(x_batch, y_batch)
            self.iters_count += 1
            self('after_batch')

    def fit(self, num_epochs):
        self.num_epochs = num_epochs

        for callback in self.callbacks:
            callback.set_runner(self)

        if self('before_fit'):       return
        for epoch in range(1, num_epochs+1):
            if self.stop: break
            self.epoch = epoch
            if self('before_epoch'): return
            if self('before_train'): return
            self.all_batches()
            if self('before_valid'): return
            self.all_batches()
            if self('after_epoch'): break
        self('after_fit')

    def __call__(self, callback_name):
        for callback in self.callbacks:
            if callback(callback_name):
                return True
        return False