# ---------------------------------------------
# | THIS FILE WAS AUTOGENERATED! DO NOT EDIT! |
# ---------------------------------------------
# edit notebooks/14_param_scheduling.ipynb and run generate_all.py

import sys
from os.path import join

sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))
from stats_logging import *

from functools import partial

def annealer(fn):
    return lambda start, end: partial(fn, start, end)

@annealer
def schedule_lin(start, end, position):
    return start + position * (end - start)

@annealer
def schedule_cos(start, end, position):
    return start + (1+math.cos(math.pi*(1-position))) * (end-start)/2

@annealer
def schedule_none(start, end, position):
    return start

@annealer
def schedule_exp(start, end, position):
    return start * (end/start)**position

def combine_schedules(segments, ranges):
    assert(sum(segments) == 1.)
    segments = tensor([0] + segments)
    assert(torch.all(segments >= 0))
    segments = torch.cumsum(segments, 0)

    def inner(pos):
        i = (pos >= segments).nonzero().max()
        actual_pos = (pos - segments[i]) / (segments[i+1] - segments[i])
        return ranges[i](actual_pos)
    return inner

def plot_schedules(schedule):
    a = torch.arange(0, 100)
    p = torch.linspace(0.01, 1, 100)
    plt.plot(a, [schedule(n) for n in p])

def one_cycle_cos(start, upper, end):
    return [schedule_cos(start, upper), schedule_cos(upper, end)]

class DynamicOpt():
    def __init__(self, parameters, **hyper_params):
        self.parameters = parameters
        self.hyper_params = dict(hyper_params)

    def __repr__(self):
        return f'(DynamicOpt) num_params: {len(self.parameters)}, hyper_params: {list(self.hyper_params)}'

    def step(self):
        for parameter in self.parameters:
            parameter.step(self.hyper_params['learning_rate'])

    def zero_grad(self):
        for parameter in self.parameters:
            parameter.zero_grad()

class Recorder(Callback):
    def __init__(self, param_names=['learning_rate']):
        self.parameters = {name: [] for name in param_names}

    def before_fit(self):
        self.losses = []

    def after_batch(self):
        if not self.model.training: return
        self.losses.append(self.loss)
        for name in self.parameters:
            self.parameters[name].append(self.optimizer.hyper_params[name])

    def plot_losses(self):
        plt.plot(self.losses)
        plt.ylabel('loss')
        plt.xlabel('batch')

    def plot_parameter(self, name):
        plt.plot(self.parameters[name])
        plt.ylabel(' '.join(name.split('_')))
        plt.xlabel('batch')

class ParamScheduler(Callback):
    def __init__(self, param_name, schedule_fn):
        self.param_name = param_name
        self.schedule_fn = schedule_fn

    def set_param(self):
        self.optimizer.hyper_params[self.param_name] = self.schedule_fn(self.iters_count / self.iters)

    def before_batch(self):
        if self.model.training:
            self.set_param()