# ---------------------------------------------
# | THIS FILE WAS AUTOGENERATED! DO NOT EDIT! |
# ---------------------------------------------
# edit notebooks/07_learning_rate_search.ipynb and run generate_all.py

import sys
from os.path import join

sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))
from early_stopping import *

class LearningRateSearch(Callback):
    def __init__(self, max_iter=1000, min_lr=1e-4, max_lr=1):
        self.max_iter = max_iter
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.best_loss = float('inf')

    def before_batch(self):
        if not self.model.training: return
        position = self.iters_count / self.iters
        learning_rate = self.min_lr * (self.max_lr/self.min_lr)**position
        self.optimizer.hyper_params['learning_rate'] = learning_rate

    def after_step(self):
        if self.iters_count >= self.max_iter or self.loss > self.best_loss*10:
            raise CancelTrainException()
        self.best_loss = min(self.best_loss, self.loss)

def plot_lr_loss(LearningRateSearch):
    losses = [l.item() for l in LearningRateSearch.losses]
    learning_rates = LearningRateSearch.parameters['learning_rate']
    plt.xscale('log')
    plt.plot(learning_rates[:len(losses)], losses[:len(losses)])
    plt.xlabel('learning rate')
    plt.ylabel('loss')