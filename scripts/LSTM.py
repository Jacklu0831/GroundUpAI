# ---------------------------------------------
# | THIS FILE WAS AUTOGENERATED! DO NOT EDIT! |
# ---------------------------------------------
# edit notebooks/26_LSTM.ipynb and run generate_all.py

import sys

sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))
from resnet import *

def init_2d_weight(shape, leak=1.):
    # default to he init
    assert len(shape) == 2
    fan = shape[0]
    gain_sq = 2.0 / (1 + leak**2)
    return torch.randn(*shape) * (gain_sq / fan)**0.5

class LSTMCell(nn.Module):
    def __init__(self, i_dim, h_dim):
        super().__init__()
        self.i_dim, self.h_dim = i_dim, h_dim
        self.Ui = nn.Parameter(init_2d_weight((i_dim, h_dim)))
        self.Uf = nn.Parameter(init_2d_weight((i_dim, h_dim)))
        self.Uo = nn.Parameter(init_2d_weight((i_dim, h_dim)))
        self.Ug = nn.Parameter(init_2d_weight((i_dim, h_dim)))
        self.Wi = nn.Parameter(init_2d_weight((h_dim, h_dim)))
        self.Wf = nn.Parameter(init_2d_weight((h_dim, h_dim)))
        self.Wo = nn.Parameter(init_2d_weight((h_dim, h_dim)))
        self.Wg = nn.Parameter(init_2d_weight((h_dim, h_dim)))

    def forward(self, x, state):
        h, c = state

        i   = (x @ self.Ui + h @ self.Wi).sigmoid()
        f   = (x @ self.Uf + h @ self.Wf).sigmoid()
        o   = (x @ self.Uo + h @ self.Wo).sigmoid()
        c_t = (x @ self.Ug + h @ self.Wg).tanh()

        c = (f*c + i*c_t).sigmoid()
        h = c.tanh() * o
        return h, (h, c)

    def __repr__(self):
        return f'LSTM({self.i_dim}, {self.h_dim})'

class LSTMLayer(nn.Module):
    def __init__(self, i_dim, h_dim):
        super().__init__()
        self.cell = LSTMCell(i_dim, h_dim)

    def forward(self, inps, state):
        outputs = []
        for inp in inps.unbind(1):
            out, state = self.cell(inp, state)
            outputs.append(out)
        return torch.stack(outputs, 1), state

    def __repr__(self):
        return f'{self.cell}'

class FastLSTMCell(nn.Module):
    def __init__(self, i_dim, h_dim):
        super().__init__()
        self.i_dim, self.h_dim = i_dim, h_dim
        # also adds a small bias
        self.x_gates = nn.Linear(i_dim, 4*h_dim)
        self.h_gates = nn.Linear(i_dim, 4*h_dim)

    def forward(self, x, state):
        h, c = state
        gates = (self.x_gates(x) + self.h_gates(h)).chunk(4, 1)

        i   = gates[0].sigmoid()
        f   = gates[1].sigmoid()
        o   = gates[2].sigmoid()
        c_t = gates[3].tanh()

        c = f*c + i*c_t
        h = o * c.tanh()
        return h, (h, c)

    def __repr__(self):
        return f'LSTM({self.i_dim}, {self.h_dim})'

class FastLSTMLayer(nn.Module):
    def __init__(self, i_dim, h_dim):
        super().__init__()
        self.cell = FastLSTMCell(i_dim, h_dim)

    def forward(self, inps, state):
        outputs = []
        for inp in inps.unbind(1):
            out, state = self.cell(inp, state)
            outputs.append(out)
        return torch.stack(outputs, 1), state

    def __repr__(self):
        return f'{self.cell}'