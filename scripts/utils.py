# ---------------------------------------------
# | THIS FILE WAS AUTOGENERATED! DO NOT EDIT! |
# ---------------------------------------------
# edit notebooks/00_utils.ipynb and run generate_all.py

from fastai import datasets
import torch
import gzip
import pickle
import matplotlib as mpl
import matplotlib.pyplot as plt
from torch import tensor
import random
import operator
import os

os.environ['KMP_DUPLICATE_LIB_OK']='True'

def process_mnist(xt, yt, xv, yv):
    '''Process mnist data with normalization'''
    xt, yt, xv, yv = map(tensor, (xt, yt, xv, yv))
    xt, xv = xt.float(), xv.float()
    mean, std = xt.mean(), xt.std()
    xt, xv = normalize(xt, mean, std), normalize(xv, mean, std)
    return xt, yt, xv, yv

mpl.rcParams['image.cmap'] = 'gray'

name2url = {'mnist': 'http://deeplearning.net/data/mnist/mnist.pkl'}
name2fn = {'mnist': process_mnist}

def get_mnist_data():
    '''Get mnist dataset'''
    return get_data('mnist')

def get_data(name):
    '''Get dataset by name'''
    if name not in name2url or name not in name2fn:
        raise Exception('Unrecognized dataset')

    path = datasets.download_data(name2url[name], ext='.gz')
    with gzip.open(path, 'rb') as f:
        ((xt, yt), (xv, yv), _) = pickle.load(f, encoding='latin-1')

    return name2fn[name](xt, yt, xv, yv)

def show_random_image(imgs):
    '''Show random image from a batch with matplotlib'''
    img = random.choice(imgs)
    if len(img.shape) == 1:
        size = int(img.shape[0] ** 0.5)
        shape = (size, size)
    else:
        shape = img.shape
    plt.imshow(img.view(shape))

def plot_by_epoch(data, label):
    '''Plot data by epoch and label data with given name'''
    plt.plot(list(range(1,len(data)+1)), data)
    plt.xlabel('epoch')
    plt.ylabel(label)
    plt.xticks(list(range(1,len(data)+1)))
    plt.show()

def test(a, b, cmp, cname=None):
    '''General two value test with selectable comparison function'''
    assert cmp(a, b), f"{cname or cmp.__name__}: \n{a}\n{b}"

def near(a, b):
    '''Function to assert tensor similarity'''
    return torch.allclose(a, b, rtol=1e-3, atol=1e-5)

def test_near(a, b):
    '''Test whether two tensors are near each other to account for floating point errors'''
    test(a, b, near)

def test_near_zero(a, tol=1e-3):
    '''Test whether tensor is near the zero tensor of it's size'''
    assert a.abs() < tol, f"Near zero: {a}"

def test_eq(a, b):
    '''Test whether two tensors are exactly equal'''
    return test(a, b, operator.eq)

def matmul_naive(a, b):
    '''Brute force matmul with 3 levels of for loop'''
    ar,ac = a.shape
    br,bc = b.shape
    assert ac==br
    c = torch.zeros(ar, bc)
    for i in range(ar):
        for j in range(bc):
            for k in range(ac):
                c[i,j] += a[i,k] * b[k,j]
    return c

def matmul_element(a, b):
    '''Sped up matmul_naive by dot producting 1d vectors'''
    ar,ac = a.shape
    br,bc = b.shape
    assert ac==br
    c = torch.zeros(ar, bc)
    for i in range(ar):
        for j in range(bc):
            c[i,j] = (a[i,:] * b[:,j]).sum()
    return c

def matmul_broadcast(a, b):
    '''Sped up matmul_element by leveraging pytorch tensor broadcasting'''
    ar,ac = a.shape
    br,bc = b.shape
    assert ac==br
    c = torch.zeros(ar, bc)
    for i in range(ar):
        c[i] = (a[i].unsqueeze(-1) * b).sum(dim=0)
    return c

def matmul_einsum(a, b):
    '''Sped up matmul_broadcast by using pytorch built-in function (einstein sum notation)'''
    return torch.einsum('ik,kj->ij', a, b)

def matmul_torch(a, b):
    '''Sped up matmul_broadcast by using pytorch built-in function (@ operator)'''
    return a@b

def normalize(x, m=None, s=None):
    '''Normalize input x with optional mean and std'''
    return (x - (m if m else x.mean())) / (s if s else x.std())