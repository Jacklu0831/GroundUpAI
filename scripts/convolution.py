# ---------------------------------------------
# | THIS FILE WAS AUTOGENERATED! DO NOT EDIT! |
# ---------------------------------------------
# edit notebooks/08_convolution.ipynb and run generate_all.py

import sys
from os.path import join
from functools import reduce
import torch.nn as nn
from torch.nn.functional import pad as torch_pad

sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))
from learning_rate_search import *

def init_conv_weight(shape, leak=0.):
    # default to he init
    assert(shape[2] == shape[3])
    # in channel * receptive field (kernel area)
    fan = shape[1] * shape[2] * shape[3]
    gain_sq = 2.0 / (1 + leak**2)
    return torch.randn(*shape) * (gain_sq / fan)**0.5

def zero_pad(inp, pad):
    return torch_pad(inp, [pad]*4, 'constant', 0)

class Reshape(Module):
    def __init__(self, shape):
        super().__init__()
        self.shape = shape

    def fwd(self, inp):
        return inp.view(-1, *self.shape)

    def bwd(self, out, inp):
        # simply reverse the fwd
        inp.g = out.g.reshape(-1, reduce(lambda x,y: x*y, self.shape))

    def __repr__(self):
        return f'Reshape{self.shape}'

class Flatten(Module):
    def __init__(self):
        super().__init__()

    def fwd(self, inp):
        self.batch_size, *self.shape = inp.shape
        return inp.view(self.batch_size, -1)

    def bwd(self, out, inp):
        inp.g = out.g.view(-1, *self.shape)

    def __repr__(self):
        return 'Flatten()'

class Conv(Module):
    def __init__(self, c_in, c_out, k_s, stride=1, pad=0, leak=1.):
        super().__init__()
        self.c_in = c_in
        self.c_out = c_out
        self.k_s = k_s
        self.stride = stride
        self.pad = pad

        self.w = Parameter(init_conv_weight((c_out, c_in, k_s, k_s), leak))
        self.b = Parameter(torch.zeros(c_out))

    def fwd(self, inp):
        batch_size, _, in_h, in_w = inp.shape
        inp = zero_pad(inp, self.pad)
        _, _, p_h, p_w = inp.shape

        # init output
        out_dim = lambda d: (d + 2 * self.pad - self.k_s) // self.stride + 1
        out = torch.zeros(batch_size, self.c_out, out_dim(in_h), out_dim(in_w))

        # compute output cell by cell
        for i in range(0, p_h - self.k_s + 1, self.stride):
            for j in range(0, p_w - self.k_s + 1, self.stride):
                receptive_field = inp[:, :, i:i+self.k_s, j:j+self.k_s].unsqueeze(1)
                out[:, :, i//self.stride, j//self.stride] = (receptive_field * self.w.data).sum((-1,-2,-3)) + self.b.data

        return out

    def bwd(self, out, inp):
        # source of var names and math calcs: https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c
        dL = out.g
        X, F, B = zero_pad(inp, self.pad), self.w.data, self.b.data
        dX, dF, dB = torch.zeros_like(X), torch.zeros_like(F), torch.zeros_like(B)
        k_s = F.shape[2]
        _, _, out_h, out_w = dL.shape

        # each cell in output are computed from a receptive field in input
        for i in range(out_h):
            for j in range(out_w):
                i_s, j_s = i * self.stride, j * self.stride
                receptive_field = X[:, :, j_s: j_s+k_s, i_s: i_s+k_s].unsqueeze(1)
                dL_section = dL[:, :, j, i][..., None, None, None]

                dX[:, :, j_s: j_s+k_s, i_s: i_s+k_s] += (F * dL_section).sum(1)
                dF += (receptive_field * dL_section).sum(0)
                dB += dL[:, :, j, i].sum(0)

        self.w.update(dF)
        self.b.update(dB)
        inp.g = dX if pad == 0 else dX[:, :, self.pad: -self.pad, self.pad: -self.pad]

    def __repr__(self): return f'Conv(in: {self.c_in}, out: {self.c_out}, kernel: {self.k_s}, stride: {self.stride}, pad: {self.pad})'