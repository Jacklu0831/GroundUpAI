{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle GAN\n",
    "source: https://arxiv.org/abs/1703.10593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys\n",
    "from os.path import join\n",
    "from fastai.vision import *\n",
    "\n",
    "sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))\n",
    "from bleu_score import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i need to have better memory of torch funcs\n",
    "# ??nn.ConvTranspose2d\n",
    "# ??nn.ReLU\n",
    "# ??nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def TConvNReLU(i, o, norm, k=3, s=2, b=True):\n",
    "    '''Tranpose convolutional layer + normalization layer + ReLU activation.\n",
    "        i: channel in\n",
    "        o: channel out\n",
    "        norm: normalization layer\n",
    "        k: kernel size\n",
    "        s: stride size\n",
    "        b: whether to have bias or not in TransposeConv\n",
    "    '''\n",
    "    return [nn.ConvTranspose2d(i, o, k, s, padding=1, output_padding=1, bias=b),\n",
    "            norm(o), \n",
    "            nn.ReLU(inplace=True)]\n",
    "\n",
    "def PadConvNReLU(i, o, norm, pad_mode, k=3, s=1, p=1, b=True, act=True, init=nn.init.kaiming_normal_):\n",
    "    '''PaddedConvolutional layer + normalization layer + ReLU activation.\n",
    "        i: channel in\n",
    "        o: channel out\n",
    "        norm: normalization layer\n",
    "        k: kernel size\n",
    "        s: stride size\n",
    "        p: padding size\n",
    "        b: bias or not in TransposeConv\n",
    "        act: whether to append ReLU after PadConvN\n",
    "        init: initialization function\n",
    "    '''\n",
    "    layers = []\n",
    "    #padding\n",
    "    if pad_mode == 'reflection':\n",
    "        layers.append(nn.ReflectionPad2d(p))\n",
    "    elif pad_mode == 'border':\n",
    "        layers.append(nn.ReplicationPad2d(p))\n",
    "    #convolution\n",
    "    conv = nn.Conv2d(i, o, k, s, p if pad_mode=='zeros' else 0, bias=b)\n",
    "    if init:\n",
    "        init(conv.weight)\n",
    "        if hasattr(conv, 'bias') and hasattr(conv.bias, 'data'):\n",
    "            conv.bias.data.fill_(0.)\n",
    "    layers.append(conv)\n",
    "    #normalization\n",
    "    layers.append(norm(o))\n",
    "    #activation\n",
    "    if act:\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, d, pad_mode, norm=None, dropout=0., b=True):\n",
    "        '''ResBlock ( y = F(x) + x).\n",
    "            pad_mode: padding mode (either \"reflection\" or \"zero\")\n",
    "            norm: normalization layer\n",
    "            dropout: dropout factor (0 to 1)\n",
    "            b: whether to have bias or not in TransposeConv\n",
    "        '''\n",
    "        super().__init__()\n",
    "        norm = norm if norm else nn.InstanceNorm2d\n",
    "        layers = PadConvNReLU(d, d, norm, pad_mode, b=b)\n",
    "        if dropout != 0: layers.append(nn.Dropout(dropout))\n",
    "        layers += PadConvNReLU(d, d, norm, pad_mode, b=b, act=False)\n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generator(i, o, c=64, norm=None, dropout=0., depth=6, pad_mode='reflection'):\n",
    "    '''Generator of GAN (resnet based).\n",
    "        i: channel in\n",
    "        o: channel out\n",
    "        c: start channel (changes are resnet is being built)\n",
    "        dropout: dropout factor (0 to 1)\n",
    "        depth: resnet depth\n",
    "        pad_mode: padding mode (either \"reflection\" or \"zero\")\n",
    "    '''\n",
    "    norm = norm if norm else nn.InstanceNorm2d\n",
    "    b = norm == nn.InstanceNorm2d\n",
    "    layers = PadConvNReLU(i, c, norm=norm, pad_mode='reflection', k=7, p=3, b=b)\n",
    "    for _ in range(2):\n",
    "        layers += PadConvNReLU(c, c*2, norm=norm, pad_mode='zeros', s=2, b=b)\n",
    "        c *= 2\n",
    "    for _ in range(depth):\n",
    "        layers.append(ResnetBlock(c, pad_mode, norm, dropout, b))\n",
    "    for _ in range(2):\n",
    "        layers += TConvNReLU(c, c//2, norm, b=b)\n",
    "        c //= 2\n",
    "    layers.append(nn.ReflectionPad2d(3))\n",
    "    layers.append(nn.Conv2d(c, o, 7, padding=0))\n",
    "    layers.append(nn.Tanh())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "  (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): ResnetBlock(\n",
       "    (conv_block): Sequential(\n",
       "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (11): ResnetBlock(\n",
       "    (conv_block): Sequential(\n",
       "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (12): ResnetBlock(\n",
       "    (conv_block): Sequential(\n",
       "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (13): ResnetBlock(\n",
       "    (conv_block): Sequential(\n",
       "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (14): ResnetBlock(\n",
       "    (conv_block): Sequential(\n",
       "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (15): ResnetBlock(\n",
       "    (conv_block): Sequential(\n",
       "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (16): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (17): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (20): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (21): ReLU(inplace=True)\n",
       "  (22): ReflectionPad2d((3, 3, 3, 3))\n",
       "  (23): Conv2d(64, 25, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (24): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_gen = generator(3, 25)\n",
    "res_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ConvNLReLU(i, o, norm, k=3, s=1, p=1, b=True, act=True, slope=0.2, init=nn.init.kaiming_normal_):\n",
    "    '''Convolutional layer + normalization layer + ReLU activation.\n",
    "        i: channel in\n",
    "        o: channel out\n",
    "        norm: normalization layer\n",
    "        k: kernel size\n",
    "        s: stride size\n",
    "        p: padding size\n",
    "        b: bias or not in TransposeConv\n",
    "        act: whether to append ReLU after PadConvN\n",
    "        slope: leaky relu slope\n",
    "        init: initialization function\n",
    "    '''\n",
    "    layers = []\n",
    "    #conv\n",
    "    conv = nn.Conv2d(i, o, k, s, p, bias=b)\n",
    "    if init:\n",
    "        init(conv.weight)\n",
    "        if hasattr(conv, 'bias') and hasattr(conv.bias, 'data'):\n",
    "            conv.bias.data.fill_(0.)\n",
    "    layers.append(conv)\n",
    "    #norm\n",
    "    if norm: layers.append(norm(o))\n",
    "    #activation\n",
    "    if act:  layers.append(nn.LeakyReLU(slope, inplace=True))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def discriminator(i, c=64, norm=None, n_layer=3, act=False):\n",
    "    '''Discriminator of GAN.\n",
    "        i: channel in\n",
    "        c: start channel (changes as we build the discriminator blocks)\n",
    "        norm: normalization layer\n",
    "        act: whether to append activation at the end of discriminator\n",
    "    '''\n",
    "    layers = []\n",
    "    norm = norm if norm else nn.InstanceNorm2d\n",
    "    b = norm == nn.InstanceNorm2d\n",
    "    layers += ConvNLReLU(i, c, None, 4, 2, 1)\n",
    "    for i in range(n_layer-1):\n",
    "        new_c = c * 2 if i <= 3 else c\n",
    "        layers += ConvNLReLU(c, new_c, norm, 4, 2, 1, b)\n",
    "        c = new_c\n",
    "    new_c = c * 2 if i <= 3 else c\n",
    "    layers += ConvNLReLU(c, new_c, norm, 4, 1, 1, b)\n",
    "    layers.append(nn.Conv2d(new_c, 1, 4, 1, 1))\n",
    "    if act: layers.append(nn.Sigmoid())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "  (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(discriminator(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CycleGAN(nn.Module):\n",
    "    def __init__(self, i, o, c=64, norm=None, gen_layers=6, dis_layers=3, dropout=0., lsgan=True):\n",
    "        '''Cycle GAN with two generators and discriminators.\n",
    "            i: channel in\n",
    "            o: channel out\n",
    "            c: start channel (dynamic)\n",
    "            norm: normalization layer\n",
    "            gen_layers: number of generator blocks\n",
    "            dis_layer: number of discriminator blocks\n",
    "            dropout: dropout factor (0 to 1)\n",
    "            lsgan: boolean factor for least square gan (https://arxiv.org/abs/1611.04076)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.G1 = generator(i, o, c, norm, dropout, gen_layers) # domain 2 to 1\n",
    "        self.G2 = generator(i, o, c, norm, dropout, gen_layers) # domain 1 to 2\n",
    "        self.D1 = discriminator(i, c, norm, dis_layers, act=not lsgan) # test domain 1\n",
    "        self.D2 = discriminator(i, c, norm, dis_layers, act=not lsgan) # test domain 2\n",
    "    \n",
    "    def forward(self, real1, real2):\n",
    "        fake1, fake2 = self.G1(real2), self.G2(real1)\n",
    "        if not self.training:\n",
    "            return torch.cat([fake_A.unsqueeze(-1), fake_B.unsqueeze(-1)], -1)\n",
    "        iden1, iden2 = self.G1(real1), self.G2(real2)\n",
    "        return [fake1, fake2, iden1, iden2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaptiveLoss(nn.Module):\n",
    "    def __init__(self, critic):\n",
    "        '''Critic wrapper for creating binary target tensors for loss computation.\n",
    "            critic: loss module\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.critic = critic\n",
    "    \n",
    "    def forward(self, out, tar, **kwargs):\n",
    "        target = out.new_ones(*output.size()) if tar else out.new_zeros(*output.size())\n",
    "        return self.critic(out, target, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CycleGANLoss(nn.Module):\n",
    "    def __init__(self, cycleGAN, w1=10., w2=10., wi=0.5, lsgan=True):\n",
    "        '''Cycle GAN loss layer with identity loss, generator loss, and discriminator loss.\n",
    "            w1: lambda factor for loss in domain 1\n",
    "            w2: lambda factor for loss in domain 2\n",
    "            wi: lambda factor for identity loss\n",
    "            lsgan: factor for https://arxiv.org/abs/1611.04076\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.cycleGAN, self.w1, self.w2, self.wi = cycleGAN, w1, w2, wi\n",
    "        self.critic = AdaptiveLoss(F.mse_loss if lsgan else F.binary_cross_entropy)\n",
    "    \n",
    "    def set_input(self, inp):\n",
    "        self.real1, self.real2 = inp\n",
    "    \n",
    "    def forward(self, out, tar):\n",
    "        fake1, fake2, iden1, iden2 = out\n",
    "        iden_loss1 = self.lambdaI * (self.lambda1 * F.l1_loss(iden1, self.real1))\n",
    "        iden_loss2 = self.lambdaI * (self.lambda2 * F.l1_loss(iden2, self.real2))\n",
    "        self.iden_loss = iden_loss1 + iden_loss2\n",
    "        \n",
    "        gen_loss1 = self.critic(self.cycleGAN.D1(fake1), True)\n",
    "        gen_loss2 = self.critic(self.cycleGAN.D2(fake2), True)\n",
    "        self.gen_loss = gen_loss1 + gen_loss2\n",
    "        \n",
    "        cycle_loss1  = self.lambda1 * F.l1_loss(self.cycleGAN.G1(fake2), self.real1)\n",
    "        cycle_loss2  = self.lambda2 * F.l1_loss(self.cycleGAN.G2(fake1), self.real2)\n",
    "        self.cycle_loss = cycle_loss1 + cycle_loss2\n",
    "        \n",
    "        return self.iden_loss + self.gen_loss + self.cycle_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CycleGANTrainer(LearnerCallback):\n",
    "    def __init__(self):\n",
    "        '''Callback for the special training procedure of Cycle GAN.'''\n",
    "        super().__init__()\n",
    "        \n",
    "    def _set_trainable(self, D1=False, D2=False):\n",
    "        gen = (not D1) and (not D2)\n",
    "        requires_grad(self.learn.model.G1, gen)\n",
    "        requires_grad(self.learn.model.G2, gen)\n",
    "        requires_grad(self.learn.model.D1, D1)\n",
    "        requires_grad(self.learn.model.D2, D2)\n",
    "        if not gen:\n",
    "            self.opt_D1.lr, self.opt_D1.mom = self.learn.opt.lr, self.learn.opt.mom\n",
    "            self.opt_D1.wd, self.opt_D1.beta = self.learn.opt.wd, self.learn.opt.beta\n",
    "            self.opt_D2.lr, self.opt_D2.mom = self.learn.opt.lr, self.learn.opt.mom\n",
    "            self.opt_D2.wd, self.opt_D2.beta = self.learn.opt.wd, self.learn.opt.beta\n",
    "    \n",
    "    def before_fit(self, **kwargs):\n",
    "        self.G1, self.G2 = self.learn.model.G1, self.learn.model.G2\n",
    "        self.D1, self.D2 = self.learn.model.D1, self.learn.model.D2\n",
    "        self.crit = self.learn.loss_func.crit\n",
    "        if not getattr(self,'opt_G',None):\n",
    "            self.opt_G = self.learn.opt.new([nn.Sequential(*flatten_model(self.G1), *flatten_model(self.G2))])\n",
    "        else: \n",
    "            self.opt_G.lr, self.opt_G.wd = self.opt.lr, self.opt.wd\n",
    "            self.opt_G.mom, self.opt_G.beta = self.opt.mom, self.opt.beta\n",
    "        if not getattr(self,'opt_D1',None):\n",
    "            self.opt_D1 = self.learn.opt.new([nn.Sequential(*flatten_model(self.D1))])\n",
    "        if not getattr(self,'opt_D2',None):\n",
    "            self.opt_D2 = self.learn.opt.new([nn.Sequential(*flatten_model(self.D2))])\n",
    "        self.learn.opt.opt = self.opt_G.opt\n",
    "        self._set_trainable()\n",
    "        self.id_smter, self.gen_smter, self.cyc_smter = SmoothenValue(0.98), SmoothenValue(0.98), SmoothenValue(0.98)\n",
    "        self.da_smter, self.db_smter = SmoothenValue(0.98), SmoothenValue(0.98)\n",
    "        self.recorder.add_metric_names(['id_loss', 'gen_loss', 'cyc_loss', 'D1_loss', 'D2_loss'])\n",
    "        \n",
    "    def before_batch(self, last_input, **kwargs):\n",
    "        self.learn.loss_func.set_input(last_input)\n",
    "    \n",
    "    def before_loss_back(self, **kwargs):\n",
    "        self.id_smter.add_value(self.loss_func.id_loss.detach().cpu())\n",
    "        self.gen_smter.add_value(self.loss_func.gen_loss.detach().cpu())\n",
    "        self.cyc_smter.add_value(self.loss_func.cyc_loss.detach().cpu())\n",
    "    \n",
    "    def after_batch(self, last_input, last_output, **kwargs):\n",
    "        self.G1.zero_grad(); self.G2.zero_grad()\n",
    "        fake1, fake2 = last_output[0].detach(), last_output[1].detach()\n",
    "        real1, real2 = last_input\n",
    "        self._set_trainable(D1=True)\n",
    "        self.D1.zero_grad()\n",
    "        loss_D1 = 0.5 * (self.crit(self.D1(real1), True) + self.crit(self.D1(fake1), False))\n",
    "        self.da_smter.add_value(loss_D1.detach().cpu())\n",
    "        loss_D1.backward()\n",
    "        self.opt_D1.step()\n",
    "        self._set_trainable(D2=True)\n",
    "        self.D2.zero_grad()\n",
    "        loss_D2 = 0.5 * (self.crit(self.D2(real2), True) + self.crit(self.D2(fake2), False))\n",
    "        self.db_smter.add_value(loss_D2.detach().cpu())\n",
    "        loss_D2.backward()\n",
    "        self.opt_D2.step()\n",
    "        self._set_trainable()\n",
    "        \n",
    "    def after_epoch(self, last_metrics, **kwargs):\n",
    "        return add_metrics(last_metrics, [s.smooth for s in [self.id_smter, self.gen_smter, self.cyc_smter, self.da_smter,self.db_smter]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CycleGAN(\n",
       "  (G1): Sequential(\n",
       "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (11): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (12): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (13): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (14): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (15): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (16): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (17): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (18): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (19): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (20): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (21): ReLU(inplace=True)\n",
       "    (22): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (23): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (26): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (27): Tanh()\n",
       "  )\n",
       "  (G2): Sequential(\n",
       "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (11): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (12): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (13): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (14): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (15): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (16): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (17): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (18): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (19): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (20): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (21): ReLU(inplace=True)\n",
       "    (22): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (23): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (26): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (27): Tanh()\n",
       "  )\n",
       "  (D1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (D2): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cycle_gan = CycleGAN(3, 3, gen_layers=9)\n",
    "cycle_gan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
