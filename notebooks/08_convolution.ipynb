{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys\n",
    "from os.path import join\n",
    "from functools import reduce\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import pad as torch_pad\n",
    "\n",
    "sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))\n",
    "from learning_rate_search import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_conv_weight(shape, leak=0.):\n",
    "    # default to he init\n",
    "    assert(shape[2] == shape[3])\n",
    "    # in channel * receptive field (kernel area)\n",
    "    fan = shape[1] * shape[2] * shape[3]\n",
    "    gain_sq = 2.0 / (1 + leak**2)\n",
    "    return torch.randn(*shape) * (gain_sq / fan)**0.5\n",
    "\n",
    "def zero_pad(inp, pad):\n",
    "    return torch_pad(inp, [pad]*4, 'constant', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_mnist_data()\n",
    "x_train = x_train[:100].view(-1, 1, 28, 28)\n",
    "torch_conv = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.47612112760543823, std: 0.9395545721054077\n"
     ]
    }
   ],
   "source": [
    "torch_conv_out_mean = 0.\n",
    "torch_conv_out_std = 0.\n",
    "for _ in range(100):\n",
    "    torch_conv.weight = nn.Parameter(init_conv_weight((4, 1, 3, 3)))\n",
    "    torch_conv_out = torch_conv(x_train).clamp_min(0.)\n",
    "    torch_conv_out_mean += torch_conv_out.mean()\n",
    "    torch_conv_out_std += torch_conv_out.std()\n",
    "torch_conv_out_mean /= 100.0\n",
    "torch_conv_out_std /= 100.0\n",
    "print(f'mean: {torch_conv_out_mean}, std: {torch_conv_out_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Reshape(Module):\n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "    \n",
    "    def fwd(self, inp): \n",
    "        return inp.view(-1, *self.shape)\n",
    "    \n",
    "    def bwd(self, out, inp): \n",
    "        # simply reverse the fwd\n",
    "        inp.g = out.g.reshape(-1, reduce(lambda x,y: x*y, self.shape))\n",
    "    \n",
    "    def __repr__(self): \n",
    "        return f'Reshape{self.shape}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Flatten(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def fwd(self, inp):\n",
    "        self.batch_size, *self.shape = inp.shape\n",
    "        return inp.view(self.batch_size, -1)\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g.view(-1, *self.shape)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Flatten()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Conv(Module):\n",
    "    def __init__(self, c_in, c_out, k_s, stride=1, pad=0, leak=1.):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.k_s = k_s\n",
    "        self.stride = stride\n",
    "        self.pad = pad   \n",
    "        \n",
    "        self.w = Parameter(init_conv_weight((c_out, c_in, k_s, k_s), leak))\n",
    "        self.b = Parameter(torch.zeros(c_out))\n",
    "    \n",
    "    def fwd(self, inp):\n",
    "        batch_size, _, in_h, in_w = inp.shape\n",
    "        inp = zero_pad(inp, self.pad)\n",
    "        _, _, p_h, p_w = inp.shape\n",
    "\n",
    "        # init output\n",
    "        out_dim = lambda d: (d + 2 * self.pad - self.k_s) // self.stride + 1\n",
    "        out = torch.zeros(batch_size, self.c_out, out_dim(in_h), out_dim(in_w))\n",
    "\n",
    "        # compute output cell by cell\n",
    "        for i in range(0, p_h - self.k_s + 1, self.stride):\n",
    "            for j in range(0, p_w - self.k_s + 1, self.stride):\n",
    "                receptive_field = inp[:, :, i:i+self.k_s, j:j+self.k_s].unsqueeze(1)\n",
    "                out[:, :, i//self.stride, j//self.stride] = (receptive_field * self.w.data).sum((-1,-2,-3)) + self.b.data\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        # source of var names and math calcs: https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c\n",
    "        dL = out.g\n",
    "        X, F, B = zero_pad(inp, self.pad), self.w.data, self.b.data\n",
    "        dX, dF, dB = torch.zeros_like(X), torch.zeros_like(F), torch.zeros_like(B)\n",
    "        k_s = F.shape[2]\n",
    "        _, _, out_h, out_w = dL.shape\n",
    "\n",
    "        # each cell in output are computed from a receptive field in input\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                i_s, j_s = i * self.stride, j * self.stride\n",
    "                receptive_field = X[:, :, j_s: j_s+k_s, i_s: i_s+k_s].unsqueeze(1)\n",
    "                dL_section = dL[:, :, j, i][..., None, None, None]\n",
    "\n",
    "                dX[:, :, j_s: j_s+k_s, i_s: i_s+k_s] += (F * dL_section).sum(1)\n",
    "                dF += (receptive_field * dL_section).sum(0)\n",
    "                dB += dL[:, :, j, i].sum(0)\n",
    "\n",
    "        self.w.update(dF)\n",
    "        self.b.update(dB)\n",
    "        inp.g = dX if pad == 0 else dX[:, :, self.pad: -self.pad, self.pad: -self.pad]\n",
    "    \n",
    "    def __repr__(self): return f'Conv(in: {self.c_in}, out: {self.c_out}, kernel: {self.k_s}, stride: {self.stride}, pad: {self.pad})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (12, 18, 37, 37)\n",
      "padding: 3\n",
      "stride: 3\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randn(12, 18, 37, 37)\n",
    "batch_size = inp.shape[0]\n",
    "c_in, c_out = inp.shape[1], 7\n",
    "k_s = 5\n",
    "pad = 3\n",
    "stride = 3\n",
    "\n",
    "print(f'input shape: {tuple(inp.shape)}')\n",
    "print(f'padding: {pad}')\n",
    "print(f'stride: {stride}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight shape (7, 18, 5, 5)\n",
      "bias shape: (7,)\n",
      "output shape: (12, 7, 13, 13)\n"
     ]
    }
   ],
   "source": [
    "torch_conv = nn.Conv2d(c_in, c_out, k_s, stride, pad)\n",
    "torch_res = torch_conv(inp)\n",
    "\n",
    "conv_layer = Conv(c_in, c_out, k_s, stride, pad)\n",
    "conv_layer.w = Parameter(torch_conv.weight)\n",
    "conv_layer.b = Parameter(torch_conv.bias)\n",
    "my_res = conv_layer.fwd(inp)\n",
    "\n",
    "print(f'weight shape {tuple(conv_layer.w.data.shape)}')\n",
    "print(f'bias shape: {tuple(conv_layer.b.data.shape)}')\n",
    "print(f'output shape: {tuple(my_res.shape)}')\n",
    "\n",
    "assert(my_res.shape == torch_res.shape)\n",
    "test_near(my_res, torch_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_mnist_data()\n",
    "x_train, y_train = x_train[:32], y_train[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(Reshape((1, 28, 28)),\n",
    "                   Conv(1, 8, 5, stride=4, pad=2, leak=0.), # 8, 7, 7 \n",
    "                   ReLU(), \n",
    "                   Conv(8, 16, 3, stride=2, pad=1, leak=0.), # 16, 4, 4\n",
    "                   Flatten(),\n",
    "                   Linear(256, 10, True))\n",
    "loss_fn = CrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Sequential)\n",
       "\t(Layer1) Reshape(1, 28, 28)\n",
       "\t(Layer2) Conv(in: 1, out: 8, kernel: 5, stride: 4, pad: 2)\n",
       "\t(Layer3) ReLU()\n",
       "\t(Layer4) Conv(in: 8, out: 16, kernel: 3, stride: 2, pad: 1)\n",
       "\t(Layer5) Flatten()\n",
       "\t(Layer6) Linear(256, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(model(x_train), y_train)\n",
    "loss_fn.backward()\n",
    "model.backward()\n",
    "\n",
    "xtg = x_train.g.clone()\n",
    "w1g = model.layers[1].w.grad.clone()\n",
    "b1g = model.layers[1].b.grad.clone()\n",
    "w2g = model.layers[3].w.grad.clone()\n",
    "b2g = model.layers[3].b.grad.clone()\n",
    "w3g = model.layers[5].w.grad.clone()\n",
    "b3g = model.layers[5].b.grad.clone()\n",
    "\n",
    "x_train2 = x_train.clone().requires_grad_(True)\n",
    "model.layers[1].w.data.requires_grad_(True)\n",
    "model.layers[1].b.data.requires_grad_(True)\n",
    "model.layers[3].w.data.requires_grad_(True)\n",
    "model.layers[3].b.data.requires_grad_(True)\n",
    "model.layers[5].w.data.requires_grad_(True)\n",
    "model.layers[5].b.data.requires_grad_(True)\n",
    "\n",
    "loss = loss_fn(model(x_train2), y_train)\n",
    "loss.backward()\n",
    "\n",
    "test_near(w1g, model.layers[1].w.data.grad)\n",
    "test_near(b1g, model.layers[1].b.data.grad)\n",
    "test_near(w2g, model.layers[3].w.data.grad)\n",
    "test_near(b2g, model.layers[3].b.data.grad)\n",
    "test_near(w3g, model.layers[5].w.data.grad)\n",
    "test_near(b3g, model.layers[5].b.data.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 50\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = get_mnist_data()\n",
    "x_train, y_train, x_valid, y_valid = x_train[:8000], y_train[:8000], x_valid[:2000], y_valid[:2000]\n",
    "\n",
    "data_bunch = get_data_bunch(x_train, y_train, x_valid, y_valid, batch_size)\n",
    "model = Sequential(Reshape((1, 28, 28)),\n",
    "                   Conv(1, 8, 5, stride=4, pad=2, leak=0.), # 8, 7, 7 \n",
    "                   ReLU(), \n",
    "                   Conv(8, 16, 3, stride=2, pad=1, leak=0.), # 16, 4, 4\n",
    "                   Flatten(),\n",
    "                   Linear(256, 10, True))\n",
    "optimizer = Optimizer(list(model.parameters()), learning_rate=learning_rate)\n",
    "loss_fn = CrossEntropy()\n",
    "learner = Learner(model, optimizer, loss_fn, data_bunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DataBunch) \n",
      "\t(DataLoader) \n",
      "\t\t(Dataset) x: (8000, 784), y: (8000,)\n",
      "\t\t(Sampler) total: 8000, batch_size: 64, shuffle: True\n",
      "\t(DataLoader) \n",
      "\t\t(Dataset) x: (2000, 784), y: (2000,)\n",
      "\t\t(Sampler) total: 2000, batch_size: 128, shuffle: False\n",
      "(Sequential)\n",
      "\t(Layer1) Reshape(1, 28, 28)\n",
      "\t(Layer2) Conv(in: 1, out: 8, kernel: 5, stride: 4, pad: 2)\n",
      "\t(Layer3) ReLU()\n",
      "\t(Layer4) Conv(in: 8, out: 16, kernel: 3, stride: 2, pad: 1)\n",
      "\t(Layer5) Flatten()\n",
      "\t(Layer6) Linear(256, 10)\n",
      "(CrossEntropy)\n",
      "(Optimizer) num_params: 6, hyper_params: ['learning_rate']\n",
      "(Callbacks) TrainEval StatsLogging\n"
     ]
    }
   ],
   "source": [
    "runner = Runner(learner, [StatsLogging([compute_accuracy])])\n",
    "print(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1\n",
      "train metrics - [0.0023859403133392335, 0.793625]\n",
      "valid metrics - [0.02583139419555664, 0.8815]\n",
      "\n",
      "Epoch - 2\n",
      "train metrics - [0.00431155252456665, 0.9125]\n",
      "valid metrics - [0.02160742950439453, 0.904]\n",
      "\n",
      "Epoch - 3\n",
      "train metrics - [0.0009750940799713135, 0.92975]\n",
      "valid metrics - [0.018961414337158203, 0.9195]\n",
      "\n",
      "Epoch - 4\n",
      "train metrics - [0.0018258848190307617, 0.943]\n",
      "valid metrics - [0.018419771194458007, 0.935]\n",
      "\n",
      "Epoch - 5\n",
      "train metrics - [0.0006376029253005981, 0.94775]\n",
      "valid metrics - [0.01726538848876953, 0.939]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runner.fit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
