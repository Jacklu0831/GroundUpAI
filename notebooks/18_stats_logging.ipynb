{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys\n",
    "sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))\n",
    "\n",
    "from learner import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we made sure training is working, time to look at the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvgStats():\n",
    "    '''Class for keeping track of average loss and other methods'''\n",
    "    def __init__(self, metrics, training): \n",
    "        self.metrics, self.training = metrics, training\n",
    "    \n",
    "    def reset(self):\n",
    "        self.count = 0\n",
    "        self.total_loss = torch.Tensor([0])\n",
    "        self.totals = [torch.Tensor([0])] * len(self.metrics)\n",
    "        \n",
    "    @property\n",
    "    def all_stats(self): return [self.total_loss] + self.totals\n",
    "    \n",
    "    @property\n",
    "    def avg_stats(self): return [s.item()/self.count for s in self.all_stats]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if not self.count: return ''\n",
    "        return f\"{'train' if self.training else 'valid'} metrics - {self.avg_stats}\"\n",
    "\n",
    "    def accumulate(self, learner):\n",
    "        batch_size = learner.x_batch.shape[0]\n",
    "        self.count += batch_size\n",
    "        self.total_loss = learner.loss * batch_size\n",
    "        for i, metric in enumerate(self.metrics):\n",
    "            self.totals[i] += metric(learner.pred, learner.y_batch) * batch_size\n",
    "\n",
    "class StatsLogging(Callback):\n",
    "    '''Callback for print out loss and other metrics for each training epoch'''\n",
    "    def __init__(self, metrics=[compute_accuracy]):\n",
    "        self.train_stats = AvgStats(metrics, True)\n",
    "        self.valid_stats = AvgStats(metrics, False)\n",
    "        \n",
    "    def before_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "        \n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.model.training else self.valid_stats\n",
    "        stats.accumulate(self.learner)\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        print(f'Epoch - {self.epoch}\\n{self.train_stats}\\n{self.valid_stats}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DataBunch) \n",
      "    (DataLoader) \n",
      "        (Dataset) x: (50000, 784), y: (50000,)\n",
      "        (Sampler) total: 50000, batch_size: 64, shuffle: True\n",
      "    (DataLoader) \n",
      "        (Dataset) x: (10000, 784), y: (10000,)\n",
      "        (Sampler) total: 10000, batch_size: 128, shuffle: False\n",
      "(Model)\n",
      "    Linear(784, 50)\n",
      "    ReLU()\n",
      "    Linear(50, 10)\n",
      "(CrossEntropy)\n",
      "(DynamicOpt) hyper_params: ['learning_rate']\n",
      "(Callbacks) ['TrainEval', 'StatsLogging']\n"
     ]
    }
   ],
   "source": [
    "data_bunch = get_data_bunch(*get_mnist_data(), batch_size=64)\n",
    "model = get_lin_model(data_bunch)\n",
    "optimizer = DynamicOpt(list(model.parameters()), learning_rate=0.1)\n",
    "loss_fn = CrossEntropy()\n",
    "callbacks = [StatsLogging()]\n",
    "learner = Learner(data_bunch, model, loss_fn, optimizer, callbacks)\n",
    "print(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1\n",
      "train metrics - [8.092031478881836e-05, 0.91324]\n",
      "valid metrics - [3.977417945861816e-05, 0.9511]\n",
      "\n",
      "Epoch - 2\n",
      "train metrics - [2.8432035446166992e-05, 0.95594]\n",
      "valid metrics - [1.3927602767944337e-05, 0.9606]\n",
      "\n",
      "Epoch - 3\n",
      "train metrics - [1.0415992736816407e-05, 0.96658]\n",
      "valid metrics - [1.2269687652587891e-05, 0.9659]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DataBunch) \n",
      "    (DataLoader) \n",
      "        (Dataset) x: (8000, 784), y: (8000,)\n",
      "        (Sampler) total: 8000, batch_size: 64, shuffle: True\n",
      "    (DataLoader) \n",
      "        (Dataset) x: (2000, 784), y: (2000,)\n",
      "        (Sampler) total: 2000, batch_size: 128, shuffle: False\n",
      "(Model)\n",
      "    Reshape(1, 28, 28)\n",
      "    Conv(1, 8, 5, 4)\n",
      "    ReLU()\n",
      "    Conv(8, 16, 3, 2)\n",
      "    Flatten()\n",
      "    Linear(256, 10)\n",
      "(CrossEntropy)\n",
      "(DynamicOpt) hyper_params: ['learning_rate']\n",
      "(Callbacks) ['TrainEval', 'StatsLogging']\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_mnist_data()\n",
    "x_train, y_train, x_valid, y_valid = x_train[:8000], y_train[:8000], x_valid[:2000], y_valid[:2000]\n",
    "\n",
    "data_bunch = get_data_bunch(x_train, y_train, x_valid, y_valid, batch_size=64)\n",
    "model = get_conv_model(data_bunch)\n",
    "optimizer = DynamicOpt(list(model.parameters()), learning_rate=0.1) # dynamic optimizer\n",
    "loss_fn = CrossEntropy()\n",
    "callbacks = [StatsLogging()]\n",
    "learner = Learner(data_bunch, model, loss_fn, optimizer, callbacks)\n",
    "print(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1\n",
      "train metrics - [0.0021502406597137453, 0.81175]\n",
      "valid metrics - [0.024427162170410157, 0.883]\n",
      "\n",
      "Epoch - 2\n",
      "train metrics - [0.0025861341953277587, 0.908625]\n",
      "valid metrics - [0.015295360565185547, 0.9115]\n",
      "\n",
      "Epoch - 3\n",
      "train metrics - [0.00155214524269104, 0.930125]\n",
      "valid metrics - [0.018342151641845703, 0.918]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DataBunch) \n",
      "    (DataLoader) \n",
      "        (Dataset) x: (8000, 784), y: (8000,)\n",
      "        (Sampler) total: 8000, batch_size: 64, shuffle: True\n",
      "    (DataLoader) \n",
      "        (Dataset) x: (2000, 784), y: (2000,)\n",
      "        (Sampler) total: 2000, batch_size: 128, shuffle: False\n",
      "(Model)\n",
      "    Reshape(1, 28, 28)\n",
      "    Conv(1, 4, 5, 2)\n",
      "    AvgPool(2, 1)\n",
      "    BatchNorm()\n",
      "    Conv(4, 16, 3, 2)\n",
      "    BatchNorm()\n",
      "    Flatten()\n",
      "    Linear(400, 64)\n",
      "    ReLU()\n",
      "    Linear(64, 10)\n",
      "(CrossEntropy)\n",
      "(DynamicOpt) hyper_params: ['learning_rate']\n",
      "(Callbacks) ['TrainEval', 'StatsLogging']\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_mnist_data()\n",
    "x_train, y_train, x_valid, y_valid = x_train[:8000], y_train[:8000], x_valid[:2000], y_valid[:2000]\n",
    "\n",
    "data_bunch = get_data_bunch(x_train, y_train, x_valid, y_valid, batch_size=64)\n",
    "model = get_conv_final_model(data_bunch)\n",
    "optimizer = DynamicOpt(list(model.parameters()), learning_rate=0.1)\n",
    "loss_fn = CrossEntropy()\n",
    "callbacks = [StatsLogging()]\n",
    "learner = Learner(data_bunch, model, loss_fn, optimizer, callbacks)\n",
    "print(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1\n",
      "train metrics - [0.0038621110916137696, 0.8345]\n",
      "valid metrics - [0.020593032836914063, 0.8885]\n",
      "\n",
      "Epoch - 2\n",
      "train metrics - [0.0014318745136260985, 0.916125]\n",
      "valid metrics - [0.016359914779663087, 0.911]\n",
      "\n",
      "Epoch - 3\n",
      "train metrics - [0.001374750018119812, 0.935875]\n",
      "valid metrics - [0.011205047607421874, 0.923]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
