{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys\n",
    "from os.path import join\n",
    "\n",
    "sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))\n",
    "from learner import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to see how the model is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvgStats():\n",
    "    def __init__(self, metrics, training): \n",
    "        self.metrics = metrics\n",
    "        self.training = training\n",
    "    \n",
    "    def reset(self):\n",
    "        self.count = 0\n",
    "        self.total_loss = torch.Tensor([0])\n",
    "        self.totals = [torch.Tensor([0])] * len(self.metrics)\n",
    "        \n",
    "    @property\n",
    "    def all_stats(self): return [self.total_loss] + self.totals\n",
    "    \n",
    "    @property\n",
    "    def avg_stats(self): return [s.item()/self.count for s in self.all_stats]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if not self.count: \n",
    "            return ''\n",
    "        return f\"{'train' if self.training else 'valid'} metrics - {self.avg_stats}\"\n",
    "\n",
    "    def accumulate(self, learner):\n",
    "        batch_size = learner.x_batch.shape[0]\n",
    "        self.count += batch_size\n",
    "        self.total_loss = learner.loss * batch_size\n",
    "        for i, metric in enumerate(self.metrics):\n",
    "            self.totals[i] += metric(learner.pred, learner.y_batch) * batch_size\n",
    "\n",
    "class StatsLogging(Callback):\n",
    "    def __init__(self, metrics=[compute_accuracy]):\n",
    "        self.train_stats = AvgStats(metrics, True)\n",
    "        self.valid_stats = AvgStats(metrics, False)\n",
    "        \n",
    "    def before_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "        \n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.model.training else self.valid_stats\n",
    "        stats.accumulate(self.learner)\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        print(f'Epoch - {self.epoch}\\n{self.train_stats}\\n{self.valid_stats}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DataBunch) \n",
      "\t(DataLoader) \n",
      "\t\t(Dataset) x: (50000, 784), y: (50000,)\n",
      "\t\t(Sampler) total: 50000, batch_size: 64, shuffle: True\n",
      "\t(DataLoader) \n",
      "\t\t(Dataset) x: (10000, 784), y: (10000,)\n",
      "\t\t(Sampler) total: 10000, batch_size: 128, shuffle: False\n",
      "(Sequential)\n",
      "\t(Layer1) Linear(784, 50)\n",
      "\t(Layer2) ReLU()\n",
      "\t(Layer3) Linear(50, 10)\n",
      "(CrossEntropy)\n",
      "(DynamicOpt) hyper_params: ['learning_rate']\n",
      "(Callbacks) ['TrainEval', 'StatsLogging']\n"
     ]
    }
   ],
   "source": [
    "data_bunch = get_data_bunch(*get_mnist_data(), batch_size=64)\n",
    "model = get_lin_model(data_bunch)\n",
    "optimizer = DynamicOpt(list(model.parameters()), learning_rate=0.1)\n",
    "loss_fn = CrossEntropy()\n",
    "callbacks = [StatsLogging()]\n",
    "learner = Learner(data_bunch, model, loss_fn, optimizer, callbacks)\n",
    "print(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1\n",
      "train metrics - [0.00016816364288330078, 0.9176]\n",
      "valid metrics - [6.626343727111816e-05, 0.9395]\n",
      "\n",
      "Epoch - 2\n",
      "train metrics - [6.021833419799805e-06, 0.9576]\n",
      "valid metrics - [1.958775520324707e-05, 0.9628]\n",
      "\n",
      "Epoch - 3\n",
      "train metrics - [4.9700145721435544e-05, 0.96766]\n",
      "valid metrics - [1.7696619033813475e-05, 0.96]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(3)s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DataBunch) \n",
      "\t(DataLoader) \n",
      "\t\t(Dataset) x: (8000, 784), y: (8000,)\n",
      "\t\t(Sampler) total: 8000, batch_size: 64, shuffle: True\n",
      "\t(DataLoader) \n",
      "\t\t(Dataset) x: (2000, 784), y: (2000,)\n",
      "\t\t(Sampler) total: 2000, batch_size: 128, shuffle: False\n",
      "(Sequential)\n",
      "\t(Layer1) Reshape(1, 28, 28)\n",
      "\t(Layer2) Conv2D(in: 1, out: 8, kernel: 5, stride: 4, pad: 2)\n",
      "\t(Layer3) ReLU()\n",
      "\t(Layer4) Conv2D(in: 8, out: 16, kernel: 3, stride: 2, pad: 1)\n",
      "\t(Layer5) Flatten()\n",
      "\t(Layer6) Linear(256, 10)\n",
      "(CrossEntropy)\n",
      "(DynamicOpt) hyper_params: ['learning_rate']\n",
      "(Callbacks) ['TrainEval', 'StatsLogging']\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_mnist_data()\n",
    "x_train, y_train, x_valid, y_valid = x_train[:8000], y_train[:8000], x_valid[:2000], y_valid[:2000]\n",
    "\n",
    "data_bunch = get_data_bunch(x_train, y_train, x_valid, y_valid, batch_size=64)\n",
    "model = get_conv_model(data_bunch)\n",
    "optimizer = DynamicOpt(list(model.parameters()), learning_rate=0.1) # dynamic optimizer\n",
    "loss_fn = CrossEntropy()\n",
    "callbacks = [StatsLogging()]\n",
    "learner = Learner(data_bunch, model, loss_fn, optimizer, callbacks)\n",
    "print(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1\n",
      "train metrics - [0.0033960378170013426, 0.805]\n",
      "valid metrics - [0.025979362487792968, 0.862]\n",
      "\n",
      "Epoch - 2\n",
      "train metrics - [0.0017266731262207031, 0.908625]\n",
      "valid metrics - [0.022161968231201173, 0.912]\n",
      "\n",
      "Epoch - 3\n",
      "train metrics - [0.0014250315427780152, 0.927125]\n",
      "valid metrics - [0.019968509674072266, 0.908]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DataBunch) \n",
      "\t(DataLoader) \n",
      "\t\t(Dataset) x: (8000, 784), y: (8000,)\n",
      "\t\t(Sampler) total: 8000, batch_size: 64, shuffle: True\n",
      "\t(DataLoader) \n",
      "\t\t(Dataset) x: (2000, 784), y: (2000,)\n",
      "\t\t(Sampler) total: 2000, batch_size: 128, shuffle: False\n",
      "(Sequential)\n",
      "\t(Layer1) Reshape(1, 28, 28)\n",
      "\t(Layer2) Conv2D(in: 1, out: 4, kernel: 5, stride: 2, pad: 1)\n",
      "\t(Layer3) AvgPool2d(kernel: 2, stride: 1, pad: 0)\n",
      "\t(Layer4) BatchNorm()\n",
      "\t(Layer5) Conv2D(in: 4, out: 16, kernel: 3, stride: 2, pad: 0)\n",
      "\t(Layer6) BatchNorm()\n",
      "\t(Layer7) Flatten()\n",
      "\t(Layer8) Linear(400, 64)\n",
      "\t(Layer9) ReLU()\n",
      "\t(Layer10) Linear(64, 10)\n",
      "(CrossEntropy)\n",
      "(DynamicOpt) hyper_params: ['learning_rate']\n",
      "(Callbacks) ['TrainEval', 'StatsLogging']\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_mnist_data()\n",
    "x_train, y_train, x_valid, y_valid = x_train[:8000], y_train[:8000], x_valid[:2000], y_valid[:2000]\n",
    "\n",
    "data_bunch = get_data_bunch(x_train, y_train, x_valid, y_valid, batch_size=64)\n",
    "model = get_conv_final_model(data_bunch)\n",
    "optimizer = DynamicOpt(list(model.parameters()), learning_rate=0.1)\n",
    "loss_fn = CrossEntropy()\n",
    "callbacks = [StatsLogging()]\n",
    "learner = Learner(data_bunch, model, loss_fn, optimizer, callbacks)\n",
    "print(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1\n",
      "train metrics - [0.0031955416202545167, 0.84525]\n",
      "valid metrics - [0.016617767333984373, 0.901]\n",
      "\n",
      "Epoch - 2\n",
      "train metrics - [0.001386306405067444, 0.925]\n",
      "valid metrics - [0.014099831581115723, 0.9225]\n",
      "\n",
      "Epoch - 3\n",
      "train metrics - [0.0022973828315734864, 0.9395]\n",
      "valid metrics - [0.012099609375, 0.931]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
