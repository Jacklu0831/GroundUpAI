{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys\n",
    "sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))\n",
    "\n",
    "from augmentation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please refer to documentation home/acknowledgement/citations for init function papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weight_he(d1, d2):\n",
    "    '''He init used for linear layer weight initialization before relu activation.\n",
    "        d1: dimension 1\n",
    "        d2: dimension 2\n",
    "    '''\n",
    "    return torch.randn(d1, d2) * (2./d1) ** 0.5\n",
    "\n",
    "def init_weight_norm(d1, d2):\n",
    "    '''init weight with N(0, 1) distribution (not suitable for relu activation).\n",
    "        d1: dimension 1\n",
    "        d2: dimension 2\n",
    "    '''\n",
    "    return torch.randn(d1, d2) / d1 ** 0.5\n",
    "\n",
    "def init_bias_zero(d):\n",
    "    '''init bias with zeros.\n",
    "        d: bias dimension\n",
    "    '''\n",
    "    return torch.zeros(d)\n",
    "\n",
    "def init_bias_norm(d):\n",
    "    '''init bias with N(0, 1) distribution.\n",
    "        d: bias dimension\n",
    "    '''\n",
    "    return torch.randn(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weight(d1, d2, end=False):\n",
    "    '''initialize linear layer weight based on whether it is follwed by ReLU activation.\n",
    "        d1: dimension 1\n",
    "        d2: dimension 2\n",
    "        end: boolean indicating whether layer is end of model (not followedb y ReLU activation)\n",
    "    '''\n",
    "    return init_weight_norm(d1, d2) if end else init_weight_he(d1, d2)\n",
    "\n",
    "def init_bias(d, zero=True):\n",
    "    '''initialize linear layer bias, default to 0 initialization.\n",
    "        d: bias dimension\n",
    "        zero: whether to simply use zero tensor for initial bias\n",
    "    '''\n",
    "    return init_bias_zero(d) if zero else init_bias_norm(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_2d_weight(shape, leak=1.):\n",
    "    '''initialize 2d weight.\n",
    "        shape: weight shape\n",
    "        leak: LReLU parameter for computing gain factor for shaping a good std\n",
    "    '''\n",
    "    assert len(shape) == 2\n",
    "    fan = shape[0]\n",
    "    gain_sq = 2.0 / (1 + leak**2)\n",
    "    return torch.randn(*shape) * (gain_sq / fan)**0.5\n",
    "\n",
    "def init_4d_weight(shape, leak=1.):\n",
    "    '''initialize 4d weight.\n",
    "        shape: weight shape\n",
    "        leak: LReLU parameter used for computing gain factor for shaping a good std\n",
    "    '''\n",
    "    assert(shape[2] == shape[3])\n",
    "    # in channel * receptive field (kernel area)\n",
    "    fan = shape[1] * shape[2] * shape[3]\n",
    "    gain_sq = 2.0 / (1 + leak**2)\n",
    "    return torch.randn(*shape) * (gain_sq / fan)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
