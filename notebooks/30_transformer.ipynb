{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "source: https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys\n",
    "from os.path import join\n",
    "from fastai.vision import *\n",
    "\n",
    "sys.path.insert(0, '/'.join(sys.path[0].split('/')[:-1] + ['scripts']))\n",
    "from cycle_gan import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PositionalEncoding(nn.Module):\n",
    "    '''Positional encoding layer using sinusoid curve'''\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.register_buffer('freq', 1 / (10000 ** (torch.arange(0., d, 2.)/d)))\n",
    "    \n",
    "    def forward(self, pos):\n",
    "        inp = torch.ger(pos, self.freq)\n",
    "        enc = torch.cat([inp.sin(), inp.cos()], dim=-1) # sinusoid\n",
    "        return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8d78f35c90>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst_encoding = PositionalEncoding(20)\n",
    "res = tst_encoding(torch.arange(0,100).float())\n",
    "plt.plot(res[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    '''Transformer embedding layer (embedding + positional encoding + dropout)'''\n",
    "    def __init__(self, vocab_size, emb_size, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.embedding = embedding(vocab_size, emb_size)\n",
    "        self.pos_encoding = PositionalEncoding(emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inp): \n",
    "        position = torch.arange(0, inp.size(1), device=inp.device).float()\n",
    "        return self.dropout(self.embedding(inp) * math.sqrt(self.emb_size) + self.pos_encoding(position))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def feed_forward(model_dim, inner_dim, drop_ff=0., double_drop=True):\n",
    "    '''Feed forward layer after multi-headed attention (2 * Linear + skip connection + layer norm)'''\n",
    "    layers = [nn.Linear(model_dim, inner_dim), nn.ReLU()]\n",
    "    if double_drop: layers.append(nn.Dropout(drop_ff))\n",
    "    layers += [nn.Linear(inner_dim, model_dim), nn.Dropout(drop_ff)]\n",
    "    return SequentialEx(*layers, MergeLayer(), nn.LayerNorm(model_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multi-Head Attention with key, query, value'''\n",
    "    def __init__(self, num_head, model_dim, head_dim=None, drop_res=0., drop_att=0., \n",
    "                 bias=True, scale=True):\n",
    "        super().__init__()\n",
    "        head_dim = head_dim if head_dim else model_dim//num_head\n",
    "        self.num_head, self.head_dim, self.scale = num_head, head_dim, scale\n",
    "        \n",
    "        # identically shaped linear layers for query, key, and value\n",
    "        self.q_lin = nn.Linear(model_dim, num_head * head_dim, bias=bias)\n",
    "        self.k_lin = nn.Linear(model_dim, num_head * head_dim, bias=bias)\n",
    "        self.v_lin = nn.Linear(model_dim, num_head * head_dim, bias=bias)\n",
    "        \n",
    "        self.out = nn.Linear(num_head * head_dim, model_dim, bias=bias)\n",
    "        self.drop_att = nn.Dropout(drop_att)\n",
    "        self.drop_res = nn.Dropout(drop_res)\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_len = q.shape[0], q.shape[1]\n",
    "        # linear pass + reshape\n",
    "        q_out, k_out, v_out = self.q_lin(q), self.k_lin(k), self.v_lin(v)\n",
    "        q_out, k_out, v_out = map(lambda x: x.view(batch_size, x.shape[1], self.num_head, self.head_dim), (q_out, k_out, v_out))\n",
    "        q_out, k_out, v_out = q_out.permute(0, 2, 1, 3), k_out.permute(0, 2, 3, 1), v_out.permute(0, 2, 1, 3)\n",
    "        # compute QxK and apply scaling + masking\n",
    "        att_scores = q_out @ k_out / self.head_dim ** 0.5 if self.scale else q_out @ k_out\n",
    "        if mask: att_scores = att_scores.masked_fill(mask, -float('inf'))\n",
    "\n",
    "        att_soft = self.drop_att(F.softmax(att_scores, dim=-1))\n",
    "        att_vec = torch.matmul(att_soft, v_out)\n",
    "        att_vec = att_vec.permute(0, 2, 1, 3).view(batch_size, seq_len, -1)\n",
    "        res = q + self.drop_res(self.out(att_vec))\n",
    "        return self.layer_norm(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransEncoder(nn.Module):\n",
    "    '''Encoder block of a Transformer model'''\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, num_head, model_dim, head_dim, inner_dim, drop_res=0., drop_att=0., \n",
    "                 drop_ff=0., bias=True, scale=True, double_drop=True):\n",
    "        super().__init__()\n",
    "        self.multiHeadAttention = MultiHeadAttention(num_head, model_dim, head_dim, drop_res, \n",
    "                                                     drop_att, bias, scale)\n",
    "        self.feed_fwd = feed_forward(model_dim, inner_dim, drop_ff, double_drop)\n",
    "    \n",
    "    def forward(self, inp, mask=None): \n",
    "        return self.feed_fwd(self.multiHeadAttention(inp, inp, inp, mask=mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransDecoder(nn.Module):\n",
    "    '''Decoder block of a Transformer model'''\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, num_head, model_dim, head_dim, inner_dim, drop_res=0., drop_att=0., \n",
    "                 drop_ff=0., bias=True, scale=True, double_drop=True):\n",
    "        super().__init__()\n",
    "        self.multiHeadAttention1 = MultiHeadAttention(num_head, model_dim, head_dim, drop_res, \n",
    "                                                      drop_att, bias, scale)\n",
    "        self.multiHeadAttention2 = MultiHeadAttention(num_head, model_dim, head_dim, drop_res,\n",
    "                                                      drop_att, bias, scale)\n",
    "        self.feed_fwd = feed_forward(model_dim, inner_dim, drop_ff, double_drop)\n",
    "    \n",
    "    def forward(self, x, enc, mask_in=None, mask_out=None): \n",
    "        y = self.multiHeadAttention1(x, x, x, mask_out)\n",
    "        return self.feed_fwd(self.multiHeadAttention2(y, enc, enc, mask_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Transformer(nn.Module):\n",
    "    '''Transformer model'''\n",
    "    def __init__(self, vocab_size, out_size, n_layers=6, num_head=8, model_dim=256, \n",
    "                 head_dim=32, inner_dim=1024, drop_inp=0.1, drop_res=0.1, drop_att=0.1, \n",
    "                 drop_ff=0.1, bias=True, scale=True, double_drop=True, pad_idx=1):\n",
    "        super().__init__()\n",
    "        self.enc_emb = TransformerEmbedding(vocab_size, model_dim, drop_inp)\n",
    "        self.dec_emb = TransformerEmbedding(out_size, model_dim, 0.)\n",
    "        self.encoder = nn.ModuleList([TransEncoder(num_head, model_dim, head_dim, inner_dim, \n",
    "                                                   drop_res, drop_att, drop_ff, bias, scale, \n",
    "                                                   double_drop) for _ in range(n_layers)])\n",
    "        self.decoder = nn.ModuleList([TransDecoder(num_head, model_dim, head_dim, inner_dim, \n",
    "                                                   drop_res, drop_att, drop_ff, bias, scale, \n",
    "                                                   double_drop) for _ in range(n_layers)])\n",
    "        self.out = nn.Linear(model_dim, out_size)\n",
    "        self.out.weight = self.dec_emb.embedding.weight\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def forward(self, inp, out):\n",
    "        mask_in  = (inp == self.pad_idx)[:,None,:,None]\n",
    "        mask_out = torch.triu(inp.new_ones(inp.size(1), inp.size(1)), diagonal=1)[None, None]\n",
    "        \n",
    "        enc_out = self.enc_emb(inp)\n",
    "        for enc_block in self.encoder: \n",
    "            enc_out = enc_block(enc_out, mask_in)\n",
    "            \n",
    "        dec_out = self.dec_emb(out)\n",
    "        for dec_block in self.decoder: \n",
    "            dec_out = dec_block(dec_out, enc_out, mask_in, mask_out)\n",
    "            \n",
    "        return self.out(dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (enc_emb): TransformerEmbedding(\n",
       "    (embedding): Embedding(1024, 256)\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dec_emb): TransformerEmbedding(\n",
       "    (embedding): Embedding(1024, 256)\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ModuleList(\n",
       "    (0): TransEncoder(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_fwd): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransEncoder(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_fwd): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransEncoder(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_fwd): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransEncoder(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_fwd): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransEncoder(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_fwd): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransEncoder(\n",
       "      (multiHeadAttention): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_fwd): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): TransDecoder(\n",
       "      (multiHeadAttention1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (multiHeadAttention2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_fwd): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransDecoder(\n",
       "      (multiHeadAttention1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (multiHeadAttention2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_fwd): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransDecoder(\n",
       "      (multiHeadAttention1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (multiHeadAttention2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_fwd): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransDecoder(\n",
       "      (multiHeadAttention1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (multiHeadAttention2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_fwd): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransDecoder(\n",
       "      (multiHeadAttention1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (multiHeadAttention2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_fwd): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransDecoder(\n",
       "      (multiHeadAttention1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (multiHeadAttention2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.1, inplace=False)\n",
       "        (drop_res): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (feed_fwd): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=256, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = Transformer(1024, 1024, model_dim=256)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
